{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ntk import empirical_ntk_ntk_vps, empirical_ntk_jacobian_contraction\n",
    "from NeuralNet import MTRAgent, ReinforceAgent\n",
    "from MtrNet import MtrNet\n",
    "import torch\n",
    "from original import OriginalMTR\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "GAME_NAME = \"Toy\"\n",
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                 SETUP AGENT and TRAINING\n",
    "\"\"\"\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "BETA_REIN_FULL = 0.1 #Betas are scalar by which we multiply bias terms in NN at initialization. They are used to control Chaos order.\n",
    "BETA_REIN_RESNET = BETA_REIN_FULL\n",
    "LEARNING_RATE_REIN = 0.0001\n",
    "HORIZON_REIN = 60\n",
    "HIDDEN_DIM_REIN = 32\n",
    "\"\"\"MATRYOSHKA\"\"\"\n",
    "BETA_MTR_FULL = 0.1\n",
    "LEARNING_RATE_MTR = 0.0001\n",
    "HORIZON_MTR= HORIZON_REIN\n",
    "HIDDEN_DIM_MTR = 32\n",
    "TAU = 1\n",
    "\"\"\"ResNet\"\"\"\n",
    "BETA_MTR_RESNET = BETA_MTR_FULL\n",
    "HIDDEN_DIM_RESNET = 16\n",
    "LEARNING_RATE_RESNET = LEARNING_RATE_MTR * 10\n",
    "\"\"\"Original\"\"\"\n",
    "HIDDEN_DIM_ORIGINAL = 16\n",
    "HORIZON_ORIGINAL = HORIZON_REIN\n",
    "BETA_ORIGINAL = 0.1\n",
    "TAU_ORIGINAL = 0.4\n",
    "LEARNING_RATE_ORIGINAL = LEARNING_RATE_RESNET\n",
    "\"\"\"Training\"\"\"\n",
    "NUM_EPOCHES = 500\n",
    "N_EPISODES = 10 # number of episodes per epoch. Used for both: Reinforce and Matryoshka\n",
    "\"\"\"\n",
    "                    INITIALIZING AGENTS\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from NeuralNet import ReinforceAgent, MTRAgent\n",
    "from MtrNet import ReinforceMtrNetAgent, MtrNetAgent, ShortLongAgent\n",
    "import torch\n",
    "\n",
    "#Reinforce Full Connected\n",
    "agent1 = ReinforceAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name=GAME_NAME, beta = BETA_MTR_FULL)\n",
    "agent1.policy.ntk_init(beta=agent1.beta)\n",
    "#Reinforce MTRNet (Custom ResNet)\n",
    "agent2 = ReinforceMtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_REIN_RESNET)\n",
    "agent2.policy.ntk_init(beta=agent2.beta)\n",
    "#MTR extra-dimension\n",
    "agent3 = MTRAgent(obs_dim, action_dim, hidden_dim= HIDDEN_DIM_MTR , horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_MTR, game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_FULL)\n",
    "agent3.policy.ntk_init(beta=agent3.beta)\n",
    "#MTR with MTRNet\n",
    "agent4 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET)\n",
    "agent4.policy.ntk_init(beta=agent4.beta)\n",
    "from original import OriginalMtrAgent\n",
    "# ORIGINAL\n",
    "agent5 = OriginalMtrAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_ORIGINAL,horizon= HORIZON_ORIGINAL, learning_rate= LEARNING_RATE_ORIGINAL , game_name= GAME_NAME, tau=TAU_ORIGINAL, beta=BETA_ORIGINAL)\n",
    "agent5.policy.ntk_init(beta=agent5.beta)\n",
    "# MTRNet with dynamical number of parameters\n",
    "agent6 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_MTR,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET, dynamical_layer_param=True)\n",
    "agent6.policy.ntk_init(beta=agent6.beta)\n",
    "#ShortLongNet\n",
    "agent7 = ShortLongAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET)\n",
    "agent7.policy.ntk_init(beta=agent7.beta)\n",
    "Agents = [agent2, agent4, agent5, agent6]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from utils import run_episodes_mtr\n",
    "episodes = run_episodes_mtr(agent3, env,n_episodes=1)\n",
    "states, _, _, _, _ = zip(*episodes[0])\n",
    "x1_3 = states[0]\n",
    "x2_3 = states[1]\n",
    "x1 = x1_3[:-1]\n",
    "x2 = x2_3[:-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      "x1: [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "x2: [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "MTR\n",
      "tensor([[[ 4.0162, -0.1050],\n",
      "         [-0.1050,  3.7462]]])\n",
      "REIN\n",
      "tensor([[[2.4769, 0.0103],\n",
      "         [0.0103, 2.4800]]])\n",
      "ReinMtrNet\n",
      "tensor([[[ 2.4379, -0.0388],\n",
      "         [-0.0388,  2.4591]]])\n",
      "MtrNet\n",
      "tensor([[[ 1.3998, -0.0497],\n",
      "         [-0.0497,  1.4191]]])\n",
      "OriginalMtr\n",
      "tensor([[[ 2.0578, -0.0929],\n",
      "         [-0.0929,  2.2116]]])\n",
      "MtrNet\n",
      "tensor([[[ 1.5493, -0.1329],\n",
      "         [-0.1329,  2.0110]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"States\")\n",
    "print(\"x1:\",x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"\")\n",
    "print(agent3.name)\n",
    "print(agent3.ntk(x1_3,x2_3)[0])\n",
    "\n",
    "print(agent1.name)\n",
    "print(agent1.ntk(x1,x2)[0])\n",
    "step = 1\n",
    "for agent in Agents:\n",
    "    print(agent.name)\n",
    "    print(agent.ntk(x1,x2, step)[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "Layer blocks.5.Q.0.weight has jacobian shape:  torch.Size([1, 2, 63])\n",
      "Layer blocks.5.Q.0.bias has jacobian shape:  torch.Size([1, 2, 7])\n",
      "Layer blocks.5.Q.2.weight has jacobian shape:  torch.Size([1, 2, 14])\n",
      "Layer blocks.5.Q.2.bias has jacobian shape:  torch.Size([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 1.5493, -0.1329],\n         [-0.1329,  2.0110]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "print(agent6.policy.count_parameters_in_block(agent6.horizon- step))\n",
    "agent6.ntk(x2,x1,step,softmax=False, show_dim_jac=True)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "Layer blocks.5.Q.0.weight has jacobian shape:  torch.Size([1, 2, 144])\n",
      "Layer blocks.5.Q.0.bias has jacobian shape:  torch.Size([1, 2, 16])\n",
      "Layer blocks.5.Q.2.weight has jacobian shape:  torch.Size([1, 2, 32])\n",
      "Layer blocks.5.Q.2.bias has jacobian shape:  torch.Size([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 0.1724, -0.1724],\n         [-0.1724,  0.1724]]], grad_fn=<SelectBackward0>)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(agent4.policy.count_parameters_in_block(agent4.horizon- step))\n",
    "agent4.ntk(x2,x1,step,softmax=True, show_dim_jac=True)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8  3.6  6.   9.6 15.6 22.8 30.  39.  48. ]\n",
      "\n",
      "[0.03, 0.06, 0.1, 0.16, 0.26, 0.38, 0.5, 0.65, 0.8]\n"
     ]
    },
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 56\n",
    "agent7.block_idx(agent7.horizon-step)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.1, 0.2, 1, 0.1, 0.2, 1, 0.1, 0.2, 1]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.1,0.2, 1] * 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE) PARALLEL\n",
    "        Remark: NOT WORKING, in process...\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 30\n",
    "N_EPISODES = 20 # ^number of episodes per epoch\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import torch.optim as optim\n",
    "from try_grad_policy import train_policy\n",
    "import multiprocessing\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "optimizer_ntk = optim.Adam(ntk_policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "\n",
    "\n",
    "initialize_env(env)\n",
    "\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "run_process1 = multiprocessing.Process(target=run_episodes, args=(policy, env,output_queue1, N_EPISODES))\n",
    "run_process2 = multiprocessing.Process(target=run_episodes, args=(ntk_policy, env,output_queue2, N_EPISODES))\n",
    "\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    run_process1.start()  # Collect episodes\n",
    "    run_process2.start()\n",
    "    # Join the processes to wait for them to finish\n",
    "    run_process1.join()\n",
    "    run_process2.join()\n",
    "    # Get the outputs from both processes\n",
    "    episodes = output_queue1.get()\n",
    "    ntk_episodes = output_queue2.get()\n",
    "    loss = train_policy(policy, optimizer, episodes)  # Update the policy based on the episodes\n",
    "    ntk_loss = train_policy(ntk_policy, optimizer_ntk, ntk_episodes)  # Update the ntk_policy based on the episodes\n",
    "    print(f'Episode {epoch + 1}, Total Loss: {BETA1}={loss} and {BETA2}={ntk_loss} ')\n",
    "close_env(env)\n",
    "\"-------------------TEST-------------------\"\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(policy, env, n_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
