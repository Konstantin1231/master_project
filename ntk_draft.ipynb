{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ntk import empirical_ntk_ntk_vps, empirical_ntk_jacobian_contraction\n",
    "from NeuralNet import MTRAgent, ReinforceAgent\n",
    "from MtrNet import MtrNet\n",
    "import torch\n",
    "from original import OriginalMTR\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "GAME_NAME = \"Toy\"\n",
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                 SETUP AGENT and TRAINING\n",
    "\"\"\"\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "BETA_REIN_FULL = 0.1 #Betas are scalar by which we multiply bias terms in NN at initialization. They are used to control Chaos order.\n",
    "BETA_REIN_RESNET = BETA_REIN_FULL\n",
    "LEARNING_RATE_REIN = 0.0001\n",
    "HORIZON_REIN = 4\n",
    "HIDDEN_DIM_REIN = 32\n",
    "\"\"\"MATRYOSHKA\"\"\"\n",
    "BETA_MTR_FULL = 0.1\n",
    "LEARNING_RATE_MTR = 0.0001\n",
    "HORIZON_MTR= 4\n",
    "HIDDEN_DIM_MTR = 32\n",
    "TAU = 1\n",
    "\"\"\"ResNet\"\"\"\n",
    "BETA_MTR_RESNET = BETA_MTR_FULL\n",
    "HIDDEN_DIM_RESNET = 16\n",
    "LEARNING_RATE_RESNET = LEARNING_RATE_MTR * 10\n",
    "\"\"\"Original\"\"\"\n",
    "HIDDEN_DIM_ORIGINAL = 16\n",
    "HORIZON_ORIGINAL = 3\n",
    "BETA_ORIGINAL = 0.1\n",
    "TAU_ORIGINAL = 0.4\n",
    "LEARNING_RATE_ORIGINAL = LEARNING_RATE_RESNET\n",
    "\"\"\"Training\"\"\"\n",
    "NUM_EPOCHES = 500\n",
    "N_EPISODES = 10 # number of episodes per epoch. Used for both: Reinforce and Matryoshka\n",
    "\"\"\"\n",
    "                    INITIALIZING AGENTS\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from NeuralNet import ReinforceAgent, MTRAgent\n",
    "from MtrNet import ReinforceMtrNetAgent, MtrNetAgent\n",
    "import torch\n",
    "\n",
    "#Reinforce Full Connected\n",
    "agent1 = ReinforceAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name=GAME_NAME, beta = BETA_MTR_FULL)\n",
    "agent1.policy.ntk_init(beta=agent1.beta)\n",
    "#Reinforce MTRNet (Custom ResNet)\n",
    "agent2 = ReinforceMtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_REIN_RESNET)\n",
    "agent2.policy.ntk_init(beta=agent2.beta)\n",
    "#MTR extra-dimension\n",
    "agent3 = MTRAgent(obs_dim, action_dim, hidden_dim= HIDDEN_DIM_MTR , horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_MTR, game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_FULL)\n",
    "agent3.policy.ntk_init(beta=agent3.beta)\n",
    "#MTR with MTRNet\n",
    "agent4 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET)\n",
    "agent4.policy.ntk_init(beta=agent4.beta)\n",
    "from original import OriginalMtrAgent\n",
    "# Set-up agent\n",
    "agent5 = OriginalMtrAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_ORIGINAL,horizon= HORIZON_ORIGINAL, learning_rate= LEARNING_RATE_ORIGINAL , game_name= GAME_NAME, tau=TAU_ORIGINAL, beta=BETA_ORIGINAL)\n",
    "agent5.policy.ntk_init(beta=agent5.beta)\n",
    "Agents = [agent2, agent4, agent5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3543, -0.3543],\n         [-0.3543,  0.3543]]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      "x1: [1.   0.   0.   0.   0.   0.   0.   0.   0.   0.75]\n",
      "x2: [0.  1.  0.  0.  0.  0.  0.  0.  0.  0.5]\n",
      "\n",
      "MTR\n",
      "tensor([[[ 0.4352, -0.4352],\n",
      "         [-0.4352,  0.4352]]])\n",
      "REIN\n",
      "tensor([[[ 0.2093, -0.2093],\n",
      "         [-0.2093,  0.2093]]])\n",
      "ReinMtrNet\n",
      "tensor([[[ 0.6880, -0.6880],\n",
      "         [-0.6880,  0.6880]]])\n",
      "MtrNet\n",
      "tensor([[[ 0.7034, -0.7034],\n",
      "         [-0.7034,  0.7034]]])\n",
      "OriginalMtr\n",
      "tensor([[[ 0.7753, -0.7753],\n",
      "         [-0.7753,  0.7753]]])\n"
     ]
    }
   ],
   "source": [
    "from utils import run_episodes_mtr\n",
    "episodes = run_episodes_mtr(agent3, env,n_episodes=1)\n",
    "states, _, _, _, _ = zip(*episodes[0])\n",
    "x1 = states[0]\n",
    "x2 = states[1]\n",
    "print(\"States\")\n",
    "print(\"x1:\",x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"\")\n",
    "print(agent3.name)\n",
    "print(agent3.ntk(x1,x2))\n",
    "x1 = x1[:-1]\n",
    "x2 = x2[:-1]\n",
    "print(agent1.name)\n",
    "print(agent1.ntk(x1,x2))\n",
    "step = 1\n",
    "for agent in Agents:\n",
    "    print(agent.name)\n",
    "    print(agent.ntk(x1,x2, step))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE) PARALLEL\n",
    "        Remark: NOT WORKING, in process...\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 30\n",
    "N_EPISODES = 20 # ^number of episodes per epoch\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import torch.optim as optim\n",
    "from try_grad_policy import train_policy\n",
    "import multiprocessing\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "optimizer_ntk = optim.Adam(ntk_policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "\n",
    "\n",
    "initialize_env(env)\n",
    "\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "run_process1 = multiprocessing.Process(target=run_episodes, args=(policy, env,output_queue1, N_EPISODES))\n",
    "run_process2 = multiprocessing.Process(target=run_episodes, args=(ntk_policy, env,output_queue2, N_EPISODES))\n",
    "\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    run_process1.start()  # Collect episodes\n",
    "    run_process2.start()\n",
    "    # Join the processes to wait for them to finish\n",
    "    run_process1.join()\n",
    "    run_process2.join()\n",
    "    # Get the outputs from both processes\n",
    "    episodes = output_queue1.get()\n",
    "    ntk_episodes = output_queue2.get()\n",
    "    loss = train_policy(policy, optimizer, episodes)  # Update the policy based on the episodes\n",
    "    ntk_loss = train_policy(ntk_policy, optimizer_ntk, ntk_episodes)  # Update the ntk_policy based on the episodes\n",
    "    print(f'Episode {epoch + 1}, Total Loss: {BETA1}={loss} and {BETA2}={ntk_loss} ')\n",
    "close_env(env)\n",
    "\"-------------------TEST-------------------\"\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(policy, env, n_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
