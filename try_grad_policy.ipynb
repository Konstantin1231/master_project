{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 4\n",
      "Action dim: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "        SETUP ENVIRONMENT\n",
    "\"\"\"\n",
    "from enviroment import *\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"State dim:\", env.observation_space.shape[0])\n",
    "print(\"Action dim:\", env.action_space.n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        SETUP AGENT\n",
    "\"\"\"\n",
    "# REINFORCE\n",
    "BETA1 = 0.1\n",
    "BETA2 = 0.9\n",
    "from try_grad_policy import PolicyNet\n",
    "policy = PolicyNet(n_inputs = env.observation_space.shape[0], n_outputs= env.action_space.n, hidden_dim=128)  # Create a policy network$\n",
    "policy.ntk_init(beta=BETA1)\n",
    "ntk_policy = PolicyNet(n_inputs = env.observation_space.shape[0], n_outputs= env.action_space.n, hidden_dim=128)  # Create a policy with ntk parametrization\n",
    "ntk_policy.ntk_init(beta=BETA2)\n",
    "#ACTOR-CRITIC\n",
    "from actor_critic import Actor, Critic\n",
    "actor = Actor(env.observation_space.shape[0], env.action_space.n)\n",
    "critic = Critic(env.observation_space.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        RUN EPISODES (NON TD)\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Collect episodes using the current policy.\n",
    "def run_episodes(policy, env, n_episodes=1):\n",
    "    run = 0\n",
    "    episodes = []\n",
    "    while(run<n_episodes):\n",
    "        episode = []\n",
    "        truncated =  False\n",
    "        terminated = False\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        while(not terminated):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            probs = policy(state_tensor)\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_state, reward, terminated, truncated, info = run_env_step(env,action=action, random_action=False)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "        run += 1\n",
    "    return episodes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta1 = 0.1    beta2 = 0.9\n",
      "Episode 1\n",
      "Loss: 39670.1107421875  and  693345.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(NUM_EPOCHES ):\n\u001B[0;32m     24\u001B[0m     episodes \u001B[38;5;241m=\u001B[39m run_episodes(policy, env, n_episodes\u001B[38;5;241m=\u001B[39mN_EPISODES)  \u001B[38;5;66;03m# Collect episodes\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m     episodes_ntk \u001B[38;5;241m=\u001B[39m \u001B[43mrun_episodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mntk_policy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_EPISODES\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Collect episodes\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     loss \u001B[38;5;241m=\u001B[39m train_policy(policy, optimizer, episodes)  \u001B[38;5;66;03m# Update the policy based on the episodes\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     loss_ntk \u001B[38;5;241m=\u001B[39m train_policy(ntk_policy, optimizer_ntk, episodes_ntk)  \u001B[38;5;66;03m# Update the policy based on the episodes\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 18\u001B[0m, in \u001B[0;36mrun_episodes\u001B[1;34m(policy, env, n_episodes)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m(\u001B[38;5;129;01mnot\u001B[39;00m terminated):\n\u001B[0;32m     17\u001B[0m     state_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(state)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m     probs \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmultinomial(probs, num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     20\u001B[0m     next_state, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m run_env_step(env,action\u001B[38;5;241m=\u001B[39maction, random_action\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\try_grad_policy.py:28\u001B[0m, in \u001B[0;36mPolicyNet.forward\u001B[1;34m(self, x, tau)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, tau\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 28\u001B[0m     logits \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(nn\u001B[38;5;241m.\u001B[39mSoftmax())(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m tau)  \u001B[38;5;66;03m# dividing by tau before\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# we apply softmax\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x1000 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAMzCAYAAABp/LlpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAna0lEQVR4nO3df2zX9Z3A8VdbbKuZrXgc5cfVcbpzblPBgfSqM8ZLZ5MZdvxxGYcLEKLz3DijNrsJ/qBzbpTbqSE5cUTmzv3jwWamWQbBcz3JsrMXMn4kmgOMYwxi1gK3s+XqRqX93B+L3XUUx7e2xfJ6PJLvH337fn8/7695iz79fPv9lhVFUQQAAEBS5Wd7AwAAAGeTKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFIrOYp+8pOfxIIFC2LGjBlRVlYWL7zwwh9ds3379vjkJz8ZVVVV8ZGPfCSeeeaZEWwVAABg9JUcRb29vTF79uxYv379Gc3/xS9+EbfcckvcdNNNsWfPnrjnnnvi9ttvjxdffLHkzQIAAIy2sqIoihEvLiuL559/PhYuXHjaOffdd19s2bIlXnvttcGxv/3bv4233nortm3bNtJLAwAAjIpJY32Bjo6OaGpqGjLW3Nwc99xzz2nXnDhxIk6cODH488DAQPz617+OP/mTP4mysrKx2ioAAPABVxRFHD9+PGbMmBHl5aPzEQljHkWdnZ1RV1c3ZKyuri56enriN7/5TZx//vmnrGlra4uHH354rLcGAABMUIcPH44/+7M/G5XnGvMoGolVq1ZFS0vL4M/d3d1xySWXxOHDh6OmpuYs7gwAADibenp6or6+Pi688MJRe84xj6Jp06ZFV1fXkLGurq6oqakZ9i5RRERVVVVUVVWdMl5TUyOKAACAUf21mjH/nqLGxsZob28fMvbSSy9FY2PjWF8aAADgjyo5iv73f/839uzZE3v27ImI333k9p49e+LQoUMR8bu3vi1dunRw/p133hkHDhyIr3zlK7Fv37548skn43vf+17ce++9o/MKAAAA3oeSo+hnP/tZXHPNNXHNNddERERLS0tcc801sXr16oiI+NWvfjUYSBERf/7nfx5btmyJl156KWbPnh2PPfZYfPvb347m5uZRegkAAAAj976+p2i89PT0RG1tbXR3d/udIgAASGws2mDMf6cIAADgg0wUAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFIbURStX78+Zs2aFdXV1dHQ0BA7dux4z/nr1q2Lj370o3H++edHfX193HvvvfHb3/52RBsGAAAYTSVH0ebNm6OlpSVaW1tj165dMXv27Ghubo4jR44MO//ZZ5+NlStXRmtra+zduzeefvrp2Lx5c9x///3ve/MAAADvV8lR9Pjjj8cXvvCFWL58eXz84x+PDRs2xAUXXBDf+c53hp3/yiuvxPXXXx+33nprzJo1K26++eZYvHjxH727BAAAMB5KiqK+vr7YuXNnNDU1/f4JysujqakpOjo6hl1z3XXXxc6dOwcj6MCBA7F169b4zGc+c9rrnDhxInp6eoY8AAAAxsKkUiYfO3Ys+vv7o66ubsh4XV1d7Nu3b9g1t956axw7diw+9alPRVEUcfLkybjzzjvf8+1zbW1t8fDDD5eyNQAAgBEZ80+f2759e6xZsyaefPLJ2LVrV/zgBz+ILVu2xCOPPHLaNatWrYru7u7Bx+HDh8d6mwAAQFIl3SmaMmVKVFRURFdX15Dxrq6umDZt2rBrHnrooViyZEncfvvtERFx1VVXRW9vb9xxxx3xwAMPRHn5qV1WVVUVVVVVpWwNAABgREq6U1RZWRlz586N9vb2wbGBgYFob2+PxsbGYde8/fbbp4RPRUVFREQURVHqfgEAAEZVSXeKIiJaWlpi2bJlMW/evJg/f36sW7cuent7Y/ny5RERsXTp0pg5c2a0tbVFRMSCBQvi8ccfj2uuuSYaGhrijTfeiIceeigWLFgwGEcAAABnS8lRtGjRojh69GisXr06Ojs7Y86cObFt27bBD184dOjQkDtDDz74YJSVlcWDDz4Yb775Zvzpn/5pLFiwIL7xjW+M3qsAAAAYobJiAryHraenJ2pra6O7uztqamrO9nYAAICzZCzaYMw/fQ4AAOCDTBQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhtRFK1fvz5mzZoV1dXV0dDQEDt27HjP+W+99VasWLEipk+fHlVVVXH55ZfH1q1bR7RhAACA0TSp1AWbN2+OlpaW2LBhQzQ0NMS6deuiubk59u/fH1OnTj1lfl9fX3z605+OqVOnxnPPPRczZ86MX/7yl3HRRReNxv4BAADel7KiKIpSFjQ0NMS1114bTzzxREREDAwMRH19fdx1112xcuXKU+Zv2LAh/umf/in27dsX55133og22dPTE7W1tdHd3R01NTUjeg4AAGDiG4s2KOntc319fbFz585oamr6/ROUl0dTU1N0dHQMu+aHP/xhNDY2xooVK6Kuri6uvPLKWLNmTfT395/2OidOnIienp4hDwAAgLFQUhQdO3Ys+vv7o66ubsh4XV1ddHZ2DrvmwIED8dxzz0V/f39s3bo1HnrooXjsscfi61//+mmv09bWFrW1tYOP+vr6UrYJAABwxsb80+cGBgZi6tSp8dRTT8XcuXNj0aJF8cADD8SGDRtOu2bVqlXR3d09+Dh8+PBYbxMAAEiqpA9amDJlSlRUVERXV9eQ8a6urpg2bdqwa6ZPnx7nnXdeVFRUDI597GMfi87Ozujr64vKyspT1lRVVUVVVVUpWwMAABiRku4UVVZWxty5c6O9vX1wbGBgINrb26OxsXHYNddff3288cYbMTAwMDj2+uuvx/Tp04cNIgAAgPFU8tvnWlpaYuPGjfHd73439u7dG1/84hejt7c3li9fHhERS5cujVWrVg3O/+IXvxi//vWv4+67747XX389tmzZEmvWrIkVK1aM3qsAAAAYoZK/p2jRokVx9OjRWL16dXR2dsacOXNi27Ztgx++cOjQoSgv/31r1dfXx4svvhj33ntvXH311TFz5sy4++6747777hu9VwEAADBCJX9P0dnge4oAAICID8D3FAEAAJxrRBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgtRFF0fr162PWrFlRXV0dDQ0NsWPHjjNat2nTpigrK4uFCxeO5LIAAACjruQo2rx5c7S0tERra2vs2rUrZs+eHc3NzXHkyJH3XHfw4MH48pe/HDfccMOINwsAADDaSo6ixx9/PL7whS/E8uXL4+Mf/3hs2LAhLrjggvjOd75z2jX9/f3x+c9/Ph5++OG49NJL39eGAQAARlNJUdTX1xc7d+6Mpqam3z9BeXk0NTVFR0fHadd97Wtfi6lTp8Ztt912Rtc5ceJE9PT0DHkAAACMhZKi6NixY9Hf3x91dXVDxuvq6qKzs3PYNT/96U/j6aefjo0bN57xddra2qK2tnbwUV9fX8o2AQAAztiYfvrc8ePHY8mSJbFx48aYMmXKGa9btWpVdHd3Dz4OHz48hrsEAAAym1TK5ClTpkRFRUV0dXUNGe/q6opp06adMv/nP/95HDx4MBYsWDA4NjAw8LsLT5oU+/fvj8suu+yUdVVVVVFVVVXK1gAAAEakpDtFlZWVMXfu3Ghvbx8cGxgYiPb29mhsbDxl/hVXXBGvvvpq7NmzZ/Dx2c9+Nm666abYs2ePt8UBAABnXUl3iiIiWlpaYtmyZTFv3ryYP39+rFu3Lnp7e2P58uUREbF06dKYOXNmtLW1RXV1dVx55ZVD1l900UUREaeMAwAAnA0lR9GiRYvi6NGjsXr16ujs7Iw5c+bEtm3bBj984dChQ1FePqa/qgQAADBqyoqiKM72Jv6Ynp6eqK2tje7u7qipqTnb2wEAAM6SsWgDt3QAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgtRFF0fr162PWrFlRXV0dDQ0NsWPHjtPO3bhxY9xwww0xefLkmDx5cjQ1Nb3nfAAAgPFUchRt3rw5WlpaorW1NXbt2hWzZ8+O5ubmOHLkyLDzt2/fHosXL46XX345Ojo6or6+Pm6++eZ488033/fmAQAA3q+yoiiKUhY0NDTEtddeG0888URERAwMDER9fX3cddddsXLlyj+6vr+/PyZPnhxPPPFELF269Iyu2dPTE7W1tdHd3R01NTWlbBcAADiHjEUblHSnqK+vL3bu3BlNTU2/f4Ly8mhqaoqOjo4zeo6333473nnnnbj44otPO+fEiRPR09Mz5AEAADAWSoqiY8eORX9/f9TV1Q0Zr6uri87OzjN6jvvuuy9mzJgxJKz+UFtbW9TW1g4+6uvrS9kmAADAGRvXT59bu3ZtbNq0KZ5//vmorq4+7bxVq1ZFd3f34OPw4cPjuEsAACCTSaVMnjJlSlRUVERXV9eQ8a6urpg2bdp7rn300Udj7dq18eMf/ziuvvrq95xbVVUVVVVVpWwNAABgREq6U1RZWRlz586N9vb2wbGBgYFob2+PxsbG06775je/GY888khs27Yt5s2bN/LdAgAAjLKS7hRFRLS0tMSyZcti3rx5MX/+/Fi3bl309vbG8uXLIyJi6dKlMXPmzGhra4uIiH/8x3+M1atXx7PPPhuzZs0a/N2jD33oQ/GhD31oFF8KAABA6UqOokWLFsXRo0dj9erV0dnZGXPmzIlt27YNfvjCoUOHorz89zegvvWtb0VfX1/8zd/8zZDnaW1tja9+9avvb/cAAADvU8nfU3Q2+J4iAAAg4gPwPUUAAADnGlEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASG1EUbR+/fqYNWtWVFdXR0NDQ+zYseM953//+9+PK664Iqqrq+Oqq66KrVu3jmizAAAAo63kKNq8eXO0tLREa2tr7Nq1K2bPnh3Nzc1x5MiRYee/8sorsXjx4rjtttti9+7dsXDhwli4cGG89tpr73vzAAAA71dZURRFKQsaGhri2muvjSeeeCIiIgYGBqK+vj7uuuuuWLly5SnzFy1aFL29vfGjH/1ocOwv//IvY86cObFhw4YzumZPT0/U1tZGd3d31NTUlLJdAADgHDIWbTCplMl9fX2xc+fOWLVq1eBYeXl5NDU1RUdHx7BrOjo6oqWlZchYc3NzvPDCC6e9zokTJ+LEiRODP3d3d0fE7/4GAAAAeb3bBCXe23lPJUXRsWPHor+/P+rq6oaM19XVxb59+4Zd09nZOez8zs7O016nra0tHn744VPG6+vrS9kuAABwjvrv//7vqK2tHZXnKimKxsuqVauG3F1666234sMf/nAcOnRo1F44DKenpyfq6+vj8OHD3qrJmHLWGC/OGuPFWWO8dHd3xyWXXBIXX3zxqD1nSVE0ZcqUqKioiK6uriHjXV1dMW3atGHXTJs2raT5ERFVVVVRVVV1ynhtba1/yBgXNTU1zhrjwlljvDhrjBdnjfFSXj563y5U0jNVVlbG3Llzo729fXBsYGAg2tvbo7Gxcdg1jY2NQ+ZHRLz00kunnQ8AADCeSn77XEtLSyxbtizmzZsX8+fPj3Xr1kVvb28sX748IiKWLl0aM2fOjLa2toiIuPvuu+PGG2+Mxx57LG655ZbYtGlT/OxnP4unnnpqdF8JAADACJQcRYsWLYqjR4/G6tWro7OzM+bMmRPbtm0b/DCFQ4cODbmVdd1118Wzzz4bDz74YNx///3xF3/xF/HCCy/ElVdeecbXrKqqitbW1mHfUgejyVljvDhrjBdnjfHirDFexuKslfw9RQAAAOeS0fvtJAAAgAlIFAEAAKmJIgAAIDVRBAAApPaBiaL169fHrFmzorq6OhoaGmLHjh3vOf/73/9+XHHFFVFdXR1XXXVVbN26dZx2ykRXylnbuHFj3HDDDTF58uSYPHlyNDU1/dGzCe8q9c+1d23atCnKyspi4cKFY7tBzhmlnrW33norVqxYEdOnT4+qqqq4/PLL/XuUM1LqWVu3bl189KMfjfPPPz/q6+vj3nvvjd/+9rfjtFsmop/85CexYMGCmDFjRpSVlcULL7zwR9ds3749PvnJT0ZVVVV85CMfiWeeeabk634gomjz5s3R0tISra2tsWvXrpg9e3Y0NzfHkSNHhp3/yiuvxOLFi+O2226L3bt3x8KFC2PhwoXx2muvjfPOmWhKPWvbt2+PxYsXx8svvxwdHR1RX18fN998c7z55pvjvHMmmlLP2rsOHjwYX/7yl+OGG24Yp50y0ZV61vr6+uLTn/50HDx4MJ577rnYv39/bNy4MWbOnDnOO2eiKfWsPfvss7Fy5cpobW2NvXv3xtNPPx2bN2+O+++/f5x3zkTS29sbs2fPjvXr15/R/F/84hdxyy23xE033RR79uyJe+65J26//fZ48cUXS7tw8QEwf/78YsWKFYM/9/f3FzNmzCja2tqGnf+5z32uuOWWW4aMNTQ0FH/3d383pvtk4iv1rP2hkydPFhdeeGHx3e9+d6y2yDliJGft5MmTxXXXXVd8+9vfLpYtW1b89V//9TjslImu1LP2rW99q7j00kuLvr6+8doi54hSz9qKFSuKv/qrvxoy1tLSUlx//fVjuk/OHRFRPP/88+855ytf+UrxiU98YsjYokWLiubm5pKuddbvFPX19cXOnTujqalpcKy8vDyampqio6Nj2DUdHR1D5kdENDc3n3Y+RIzsrP2ht99+O9555524+OKLx2qbnANGeta+9rWvxdSpU+O2224bj21yDhjJWfvhD38YjY2NsWLFiqirq4srr7wy1qxZE/39/eO1bSagkZy16667Lnbu3Dn4FrsDBw7E1q1b4zOf+cy47JkcRqsLJo3mpkbi2LFj0d/fH3V1dUPG6+rqYt++fcOu6ezsHHZ+Z2fnmO2TiW8kZ+0P3XfffTFjxoxT/uGD/28kZ+2nP/1pPP3007Fnz55x2CHnipGctQMHDsS///u/x+c///nYunVrvPHGG/GlL30p3nnnnWhtbR2PbTMBjeSs3XrrrXHs2LH41Kc+FUVRxMmTJ+POO+/09jlG1em6oKenJ37zm9/E+eeff0bPc9bvFMFEsXbt2ti0aVM8//zzUV1dfba3wznk+PHjsWTJkti4cWNMmTLlbG+Hc9zAwEBMnTo1nnrqqZg7d24sWrQoHnjggdiwYcPZ3hrnmO3bt8eaNWviySefjF27dsUPfvCD2LJlSzzyyCNne2twirN+p2jKlClRUVERXV1dQ8a7urpi2rRpw66ZNm1aSfMhYmRn7V2PPvporF27Nn784x/H1VdfPZbb5BxQ6ln7+c9/HgcPHowFCxYMjg0MDERExKRJk2L//v1x2WWXje2mmZBG8ufa9OnT47zzzouKiorBsY997GPR2dkZfX19UVlZOaZ7ZmIayVl76KGHYsmSJXH77bdHRMRVV10Vvb29cccdd8QDDzwQ5eX+3zzv3+m6oKam5ozvEkV8AO4UVVZWxty5c6O9vX1wbGBgINrb26OxsXHYNY2NjUPmR0S89NJLp50PESM7axER3/zmN+ORRx6Jbdu2xbx588Zjq0xwpZ61K664Il599dXYs2fP4OOzn/3s4Cfp1NfXj+f2mUBG8ufa9ddfH2+88cZgeEdEvP766zF9+nRBxGmN5Ky9/fbbp4TPuzH+u9+hh/dv1LqgtM+AGBubNm0qqqqqimeeeab4r//6r+KOO+4oLrrooqKzs7MoiqJYsmRJsXLlysH5//Ef/1FMmjSpePTRR4u9e/cWra2txXnnnVe8+uqrZ+slMEGUetbWrl1bVFZWFs8991zxq1/9avBx/Pjxs/USmCBKPWt/yKfPcaZKPWuHDh0qLrzwwuLv//7vi/379xc/+tGPiqlTpxZf//rXz9ZLYIIo9ay1trYWF154YfGv//qvxYEDB4p/+7d/Ky677LLic5/73Nl6CUwAx48fL3bv3l3s3r27iIji8ccfL3bv3l388pe/LIqiKFauXFksWbJkcP6BAweKCy64oPiHf/iHYu/evcX69euLioqKYtu2bSVd9wMRRUVRFP/8z/9cXHLJJUVlZWUxf/784j//8z8H/9qNN95YLFu2bMj8733ve8Xll19eVFZWFp/4xCeKLVu2jPOOmahKOWsf/vCHi4g45dHa2jr+G2fCKfXPtf9PFFGKUs/aK6+8UjQ0NBRVVVXFpZdeWnzjG98oTp48Oc67ZiIq5ay98847xVe/+tXisssuK6qrq4v6+vriS1/6UvE///M/479xJoyXX3552P/2evdsLVu2rLjxxhtPWTNnzpyisrKyuPTSS4t/+Zd/Kfm6ZUXh/iUAAJDXWf+dIgAAgLNJFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApPZ/ENEBIzjDHuYAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE)\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 30\n",
    "N_EPISODES = 5 # ^number of episodes per epoch\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import torch.optim as optim\n",
    "from try_grad_policy import train_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "optimizer_ntk = optim.Adam(ntk_policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "initialize_env(env)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "loss_list = []\n",
    "loss_ntk_list = []\n",
    "# Train for a number of epochs\n",
    "print(f\"beta1 = {BETA1}    beta2 = {BETA2}\")\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    episodes = run_episodes(policy, env, n_episodes=N_EPISODES)  # Collect episodes\n",
    "    episodes_ntk = run_episodes(ntk_policy, env, n_episodes=N_EPISODES)  # Collect episodes\n",
    "\n",
    "    loss = train_policy(policy, optimizer, episodes)  # Update the policy based on the episodes\n",
    "    loss_ntk = train_policy(ntk_policy, optimizer_ntk, episodes_ntk)  # Update the policy based on the episodes\n",
    "    loss_list.append(loss)\n",
    "    loss_ntk_list.append(loss_ntk)\n",
    "    print(f\"Episode {epoch + 1}\")\n",
    "    print(f'Loss: {loss}  and  {loss_ntk}')\n",
    "close_env(env)\n",
    "ax1.scatter(loss_list, label=f\"{BETA1}\")\n",
    "ax1.scatter(loss_ntk_list, label=f\"{BETA2}\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koq1231\\PycharmProjects\\RL\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST AGENT\"\"\"\n",
    "\"-------------------TEST-------------------\"\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(policy, env, n_episodes=3)\n",
    "close_env(env)\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(ntk_policy, env, n_episodes=3)\n",
    "close_env(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE)\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 30\n",
    "N_EPISODES = 20 # ^number of episodes per epoch\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import torch.optim as optim\n",
    "from try_grad_policy import train_policy\n",
    "import multiprocessing\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "optimizer_ntk = optim.Adam(ntk_policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "\n",
    "\n",
    "initialize_env(env)\n",
    "\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "run_process1 = multiprocessing.Process(target=run_episodes, args=(policy, env,output_queue1, N_EPISODES))\n",
    "run_process2 = multiprocessing.Process(target=run_episodes, args=(ntk_policy, env,output_queue2, N_EPISODES))\n",
    "\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    run_process1.start()  # Collect episodes\n",
    "    run_process2.start()\n",
    "    # Join the processes to wait for them to finish\n",
    "    run_process1.join()\n",
    "    run_process2.join()\n",
    "    # Get the outputs from both processes\n",
    "    episodes = output_queue1.get()\n",
    "    ntk_episodes = output_queue2.get()\n",
    "    loss = train_policy(policy, optimizer, episodes)  # Update the policy based on the episodes\n",
    "    ntk_loss = train_policy(ntk_policy, optimizer_ntk, ntk_episodes)  # Update the ntk_policy based on the episodes\n",
    "    print(f'Episode {epoch + 1}, Total Loss: {BETA1}={loss} and {BETA2}={ntk_loss} ')\n",
    "close_env(env)\n",
    "\"-------------------TEST-------------------\"\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(policy, env, n_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        RUN EPISODES (TD)\n",
    "\"\"\"\n",
    "# We do online update at each step\n",
    "import torch.optim as optim\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.01)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.01)\n",
    "import numpy as np\n",
    "\n",
    "def collect_and_update(actor,critic, env, n_episodes=10):\n",
    "    run = 0\n",
    "    gamma = 0.99\n",
    "    rewards = []\n",
    "    while(run<n_episodes):\n",
    "        reward_ = 0\n",
    "        truncated =  False\n",
    "        terminated = False\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        while(not terminated):\n",
    "            # Actor decides action\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            probs = actor(state_tensor)\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # Take action in the environment\n",
    "            next_state, reward, terminated, truncated, info = run_env_step(env,action=action, random_action=False)\n",
    "\n",
    "\n",
    "            # Critic evaluates the action\n",
    "            state_value = critic(state_tensor)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            next_state_value = critic(next_state_tensor)\n",
    "            target = reward + gamma * next_state_value\n",
    "            td_error = target - state_value\n",
    "\n",
    "\n",
    "            # Update Critic\n",
    "            critic_loss = td_error.pow(2)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor using the Advantage (TD Error)\n",
    "            actor_loss = -torch.log(probs[0, action]) * td_error.detach()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            state = next_state\n",
    "            reward_ += 1\n",
    "        rewards.append(reward_)\n",
    "\n",
    "        run += 1\n",
    "    return np.mean(rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (ACTOR-CRITIC)\n",
    "\"\"\"\n",
    "NUM_EPOCHES = 100\n",
    "initialize_env(env)\n",
    "# Train for a number of epochs\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    result=collect_and_update(actor,critic,env,10)\n",
    "    print(\"Epoch: \", epoch, \"Reward: \", result)\n",
    "close_env(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
