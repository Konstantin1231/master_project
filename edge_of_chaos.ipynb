{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        SETUP ENVIRONMENT\n",
    "\n",
    "Choose a game:\n",
    "- \"Cart\" # ok for 100 epoch\n",
    "- \"Car\" # not good choice\n",
    "- \"Pendulum\" # not good choice\n",
    "- \"Lake\" # demand more then 1500 epoch\n",
    "- \"Maze\" # 100 epoch is OK\n",
    "- \"Toy\" # 100 epoch\n",
    "\"\"\"\n",
    "GAME_NAME = \"Toy\"\n",
    "HORIZON = 7 #fixed horizon\n",
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "from utils import train_agent\n",
    "from utils import Ntk_analysis\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )\n",
    "env.rnd_init_state = False\n",
    "env.testing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                 SETUP AGENT and TRAINING\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "HIDDEN_DIM_REIN = 24\n",
    "EPSILON = 0.15 # for apsilon greedy policy\n",
    "\"\"\"MATRYOSHKA\"\"\"\n",
    "HIDDEN_DIM_MTR = HIDDEN_DIM_REIN\n",
    "\"\"\"ResNet\"\"\"\n",
    "HIDDEN_DIM_RESNET = 16\n",
    "#MTRNet with dynamically changing parameters number per layer\n",
    "INIT_HIDDEN_LAYER = 20\n",
    "\n",
    "\"\"\"Training\"\"\"\n",
    "TAU = 0.6\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHES = 400\n",
    "N_EPISODES = 40 # number of episodes per epoch.\n",
    "\n",
    "\"\"\"Testing\"\"\"\n",
    "NUM_TEST_EPISODES = 300\n",
    "\n",
    "\n",
    "\"\"\"DYNAMICAL TAU AND LR\"\"\"\n",
    "PATIENCE = 20 # for dynamical tau and lr\n",
    "TAU_END = TAU # for dynamical tau\n",
    "LR_END = LEARNING_RATE / 2000 # for dynamical learning rate\n",
    "\"\"\"\n",
    "                    INITIALIZING AGENTS\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "#more settings---------------------\n",
    "HORIZON_REIN = HORIZON\n",
    "HORIZON_MTR = HORIZON\n",
    "LEARNING_RATE_REIN = LEARNING_RATE\n",
    "LEARNING_RATE_MTR  = LEARNING_RATE\n",
    "LEARNING_RATE_RESNET = LEARNING_RATE\n",
    "LEARNING_RATE_ORIGINAL = LEARNING_RATE\n",
    "#-------------------------------------\n",
    "HIDDEN_DIM_ORIGINAL = HIDDEN_DIM_RESNET\n",
    "from original import OriginalMtrAgent\n",
    "from NeuralNet import ReinforceAgent, MTRAgent\n",
    "from MtrNet import ReinforceMtrNetAgent, MtrNetAgent, ShortLongAgent\n",
    "import torch\n",
    "\n",
    "#MTR with MTRNet\n",
    "agent4 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU)\n",
    "\n",
    "#Original MTR\n",
    "agent5 = OriginalMtrAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_ORIGINAL,horizon= HORIZON, learning_rate= LEARNING_RATE_ORIGINAL , game_name= GAME_NAME, tau=TAU)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"DataFrame to store results\"\"\"\n",
    "import pandas as pd\n",
    "FROM_PATH = \"optimality_test.csv\"\n",
    "TO_PATH = \"optimality_test.csv\"\n",
    "df = pd.read_csv(FROM_PATH)\n",
    "data1 = {\"init\": \"eoc\", 'description': \"Toy, hor: 7. \"}\n",
    "\n",
    "# function to push data\n",
    "def push(df,data2,data3,data1=data1):\n",
    "    data1.update(data2)\n",
    "    data1.update(data3)\n",
    "    df = df._append(data1, ignore_index=True)\n",
    "    return df\n",
    "#load to csv file path\n",
    "def load(df,path=TO_PATH):\n",
    "    df.to_csv(path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated until horizon 7\n",
      "0.5627487176804834\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check the optimal solution\"\"\"\n",
    "env.generate_all_q_stars(HORIZON, tau =0.3)\n",
    "v_star = env.v_star[str(HORIZON)]\n",
    "opt_sol = v_star[0]\n",
    "print(opt_sol)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  game        agent init      lr  tau      test  \\\n0  Lol          NaN  eoc  0.0000  0.0  0.000000   \n1  Toy  OriginalMtr  eoc  0.0001  0.6  0.639667   \n2  Toy  OriginalMtr  eoc  0.0001  0.6  0.641333   \n\n                                               train  \\\n0                                                0.0   \n1  [-0.017999999999999967, 0.17399999999999977, 0...   \n2  [0.009999999999999998, 0.062499999999999986, 0...   \n\n                                       train_entropy  full_rank  total_params  \\\n0                                                  0          0             0   \n1  [-0.5984274499858732, -0.38571739031374463, -0...          1          3038   \n2  [-0.5156363041698933, -0.42130479626357553, -0...          1          3038   \n\n    opt_sol    description   opt_sol  \n0       0.0            NaN       NaN  \n1       NaN  Toy, hor: 7.   0.371557  \n2       NaN  Toy, hor: 7.   0.371557  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>game</th>\n      <th>agent</th>\n      <th>init</th>\n      <th>lr</th>\n      <th>tau</th>\n      <th>test</th>\n      <th>train</th>\n      <th>train_entropy</th>\n      <th>full_rank</th>\n      <th>total_params</th>\n      <th>opt_sol</th>\n      <th>description</th>\n      <th>opt_sol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lol</td>\n      <td>NaN</td>\n      <td>eoc</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Toy</td>\n      <td>OriginalMtr</td>\n      <td>eoc</td>\n      <td>0.0001</td>\n      <td>0.6</td>\n      <td>0.639667</td>\n      <td>[-0.017999999999999967, 0.17399999999999977, 0...</td>\n      <td>[-0.5984274499858732, -0.38571739031374463, -0...</td>\n      <td>1</td>\n      <td>3038</td>\n      <td>NaN</td>\n      <td>Toy, hor: 7.</td>\n      <td>0.371557</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Toy</td>\n      <td>OriginalMtr</td>\n      <td>eoc</td>\n      <td>0.0001</td>\n      <td>0.6</td>\n      <td>0.641333</td>\n      <td>[0.009999999999999998, 0.062499999999999986, 0...</td>\n      <td>[-0.5156363041698933, -0.42130479626357553, -0...</td>\n      <td>1</td>\n      <td>3038</td>\n      <td>NaN</td>\n      <td>Toy, hor: 7.</td>\n      <td>0.371557</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 1\n",
      "tau 0.600\n",
      "Learning rate 0.90936 * 10^-3\n",
      "Reward: -0.3949999999999999\n",
      "Entropy Reward: -0.8330426853196696\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.0700000000000002\n",
      "\n",
      "Epoch 50\n",
      "tau 0.600\n",
      "Learning rate 0.75466 * 10^-3\n",
      "Reward: 0.5975\n",
      "Entropy Reward: 0.3448601762671024\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.5700000000000001\n",
      "\n",
      "Epoch 99\n",
      "tau 0.600\n",
      "Learning rate 0.62919 * 10^-3\n",
      "Reward: 0.6475000000000005\n",
      "Entropy Reward: 0.3651392852328719\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6462499999999995\n",
      "\n",
      "Epoch 148\n",
      "tau 0.600\n",
      "Learning rate 0.48304 * 10^-3\n",
      "Reward: 0.6350000000000005\n",
      "Entropy Reward: 0.3687478426750751\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6500000000000006\n",
      "\n",
      "Epoch 197\n",
      "tau 0.600\n",
      "Learning rate 0.40722 * 10^-3\n",
      "Reward: 0.6625000000000005\n",
      "Entropy Reward: 0.34279616901767446\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6962499999999998\n",
      "\n",
      "Epoch 246\n",
      "tau 0.600\n",
      "Learning rate 0.31771 * 10^-3\n",
      "Reward: 0.6249999999999999\n",
      "Entropy Reward: 0.3567016973043792\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7387500000000002\n",
      "\n",
      "Epoch 295\n",
      "tau 0.600\n",
      "Learning rate 0.27064 * 10^-3\n",
      "Reward: 0.7825000000000004\n",
      "Entropy Reward: 0.3343510915106162\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7050000000000011\n",
      "\n",
      "Epoch 344\n",
      "tau 0.600\n",
      "Learning rate 0.21437 * 10^-3\n",
      "Reward: 0.7175000000000004\n",
      "Entropy Reward: 0.3424265093763825\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7537500000000008\n",
      "\n",
      "Epoch 393\n",
      "tau 0.600\n",
      "Learning rate 0.18440 * 10^-3\n",
      "Reward: 0.7575000000000003\n",
      "Entropy Reward: 0.33741417357698095\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7000000000000011\n",
      "\n",
      "Epoch 442\n",
      "tau 0.600\n",
      "Learning rate 0.14815 * 10^-3\n",
      "Reward: 0.7050000000000001\n",
      "Entropy Reward: 0.34882806596113375\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6725000000000005\n",
      "\n",
      "Epoch 491\n",
      "tau 0.600\n",
      "Learning rate 0.12862 * 10^-3\n",
      "Reward: 0.7175000000000004\n",
      "Entropy Reward: 0.3385028427769431\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6350000000000008\n",
      "\n",
      "Epoch 540\n",
      "tau 0.600\n",
      "Learning rate 0.11205 * 10^-3\n",
      "Reward: 0.6975000000000003\n",
      "Entropy Reward: 0.3479835701663978\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7437500000000006\n",
      "\n",
      "Epoch 589\n",
      "tau 0.600\n",
      "Learning rate 0.09170 * 10^-3\n",
      "Reward: 0.710000000000001\n",
      "Entropy Reward: 0.3449995760683669\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7312500000000003\n",
      "\n",
      "Epoch 638\n",
      "tau 0.600\n",
      "Learning rate 0.08056 * 10^-3\n",
      "Reward: 0.7350000000000005\n",
      "Entropy Reward: 0.3440761246392504\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6425000000000006\n",
      "\n",
      "Epoch 687\n",
      "tau 0.600\n",
      "Learning rate 0.06674 * 10^-3\n",
      "Reward: 0.6875000000000003\n",
      "Entropy Reward: 0.33266144046792767\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6362500000000012\n",
      "\n",
      "Epoch 736\n",
      "tau 0.600\n",
      "Learning rate 0.05910 * 10^-3\n",
      "Reward: 0.695000000000001\n",
      "Entropy Reward: 0.3461657822004055\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6575000000000003\n",
      "\n",
      "Epoch 785\n",
      "tau 0.600\n",
      "Learning rate 0.04953 * 10^-3\n",
      "Reward: 0.6450000000000002\n",
      "Entropy Reward: 0.33796605184208606\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6062500000000005\n",
      "\n",
      "Epoch 834\n",
      "tau 0.600\n",
      "Learning rate 0.04418 * 10^-3\n",
      "Reward: 0.7000000000000008\n",
      "Entropy Reward: 0.3479868386173621\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6737500000000003\n",
      "\n",
      "Epoch 883\n",
      "tau 0.600\n",
      "Learning rate 0.03743 * 10^-3\n",
      "Reward: 0.7400000000000005\n",
      "Entropy Reward: 0.34344870674889544\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6775000000000009\n",
      "\n",
      "Epoch 932\n",
      "tau 0.600\n",
      "Learning rate 0.03362 * 10^-3\n",
      "Reward: 0.5875000000000006\n",
      "Entropy Reward: 0.34183370650280265\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6900000000000007\n",
      "\n",
      "Epoch 981\n",
      "tau 0.600\n",
      "Learning rate 0.02877 * 10^-3\n",
      "Reward: 0.6575000000000008\n",
      "Entropy Reward: 0.34747138431295765\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7512500000000001\n",
      "\n",
      "Epoch 1030\n",
      "tau 0.600\n",
      "Learning rate 0.02601 * 10^-3\n",
      "Reward: 0.7250000000000002\n",
      "Entropy Reward: 0.353239450021647\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6700000000000004\n",
      "\n",
      "Epoch 1079\n",
      "tau 0.600\n",
      "Learning rate 0.02358 * 10^-3\n",
      "Reward: 0.73\n",
      "Entropy Reward: 0.34563321193971214\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6962500000000007\n",
      "\n",
      "Epoch 1128\n",
      "tau 0.600\n",
      "Learning rate 0.02045 * 10^-3\n",
      "Reward: 0.6300000000000001\n",
      "Entropy Reward: 0.341520456911967\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.701250000000001\n",
      "\n",
      "Epoch 1177\n",
      "tau 0.600\n",
      "Learning rate 0.01864 * 10^-3\n",
      "Reward: 0.7675000000000001\n",
      "Entropy Reward: 0.34707963566441336\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7100000000000002\n",
      "\n",
      "Epoch 1226\n",
      "tau 0.600\n",
      "Learning rate 0.01631 * 10^-3\n",
      "Reward: 0.7375000000000005\n",
      "Entropy Reward: 0.3419340248903608\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6587500000000003\n",
      "\n",
      "Epoch 1275\n",
      "tau 0.600\n",
      "Learning rate 0.01495 * 10^-3\n",
      "Reward: 0.7900000000000003\n",
      "Entropy Reward: 0.34620056740008304\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7575000000000012\n",
      "\n",
      "Epoch 1324\n",
      "tau 0.600\n",
      "Learning rate 0.01319 * 10^-3\n",
      "Reward: 0.6975000000000005\n",
      "Entropy Reward: 0.34289318305905925\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6587500000000001\n",
      "\n",
      "Epoch 1373\n",
      "tau 0.600\n",
      "Learning rate 0.01216 * 10^-3\n",
      "Reward: 0.6000000000000002\n",
      "Entropy Reward: 0.3456939320778475\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6400000000000003\n",
      "\n",
      "Epoch 1422\n",
      "tau 0.600\n",
      "Learning rate 0.01080 * 10^-3\n",
      "Reward: 0.7550000000000007\n",
      "Entropy Reward: 0.34670836546691136\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7025000000000006\n",
      "\n",
      "Epoch 1471\n",
      "tau 0.600\n",
      "Learning rate 0.01001 * 10^-3\n",
      "Reward: 0.6550000000000005\n",
      "Entropy Reward: 0.3430900301970542\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7100000000000007\n",
      "\n",
      "Epoch 1520\n",
      "tau 0.600\n",
      "Learning rate 0.00929 * 10^-3\n",
      "Reward: 0.6325000000000005\n",
      "Entropy Reward: 0.3479206322389655\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.7150000000000005\n",
      "\n",
      "Epoch 1569\n",
      "tau 0.600\n",
      "Learning rate 0.00834 * 10^-3\n",
      "Reward: 0.7875000000000003\n",
      "Entropy Reward: 0.3442060526111165\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.3715566147907546\n",
      "CK full rank: True\n",
      "Test reward: 0.6850000000000007\n",
      "\n",
      "Calculated until horizon 7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Matryoshka \"ORIGINAL\" Training\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent5.policy.ntk_init()\n",
    "agent5.tau = TAU\n",
    "agent5.lr = LEARNING_RATE\n",
    "_,_,data5 = train_agent(agent5, env, num_epoches=NUM_EPOCHES*4, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent5.tau, lr_end=LR_END, patience=PATIENCE, clip_grad=False, testing=True)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data5_ = test_agent(agent5, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES, tau=agent5.tau)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "load(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "ck = Ntk_analysis(env)\n",
    "ck.full_analysis(agent5, mode = \"ck\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ck.rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated until horizon 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'1': {'overall': 0.10441570319714011,\n  'max': 0.05092001038908558,\n  'mean': 0.026103925799285027},\n '2': {'overall': 0.0922388014922231,\n  'max': 0.05087902263453494,\n  'mean': 0.023059700373055776},\n '3': {'overall': 0.07993470315002459,\n  'max': 0.05345913956517403,\n  'mean': 0.019983675787506146},\n '4': {'overall': 0.039458952234961764,\n  'max': 0.039458952234961764,\n  'mean': 0.009864738058740441}}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck.genarate_optimal_Q(agent5)\n",
    "ck.policy_eval()\n",
    "ck.m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 1\n",
      "tau 0.400\n",
      "Learning rate 0.88101 * 10^-3\n",
      "Reward: 0.8275000000000003\n",
      "Entropy Reward: -0.34565317362081255\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: False\n",
      "Test reward: 0.8637500000000016\n",
      "\n",
      "Epoch 50\n",
      "tau 0.400\n",
      "Learning rate 0.68815 * 10^-3\n",
      "Reward: 0.6625000000000001\n",
      "Entropy Reward: -0.06710230871103698\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: False\n",
      "Test reward: 0.6087500000000013\n",
      "\n",
      "Epoch 99\n",
      "tau 0.400\n",
      "Learning rate 0.54192 * 10^-3\n",
      "Reward: 0.6900000000000005\n",
      "Entropy Reward: 0.19275217301677916\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.5900000000000003\n",
      "\n",
      "Epoch 148\n",
      "tau 0.400\n",
      "Learning rate 0.38433 * 10^-3\n",
      "Reward: 0.5650000000000002\n",
      "Entropy Reward: 0.1699173265695572\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.5437500000000001\n",
      "\n",
      "Epoch 197\n",
      "tau 0.400\n",
      "Learning rate 0.30854 * 10^-3\n",
      "Reward: 0.4824999999999998\n",
      "Entropy Reward: 0.3026730555039831\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.5425000000000006\n",
      "\n",
      "Epoch 246\n",
      "tau 0.400\n",
      "Learning rate 0.22496 * 10^-3\n",
      "Reward: 0.5875000000000001\n",
      "Entropy Reward: 0.32378367721103135\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6250000000000012\n",
      "\n",
      "Epoch 295\n",
      "tau 0.400\n",
      "Learning rate 0.18382 * 10^-3\n",
      "Reward: 0.6099999999999999\n",
      "Entropy Reward: 0.3641328911378515\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6100000000000003\n",
      "\n",
      "Epoch 344\n",
      "tau 0.400\n",
      "Learning rate 0.13749 * 10^-3\n",
      "Reward: 0.6900000000000005\n",
      "Entropy Reward: 0.38778931839246084\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7050000000000003\n",
      "\n",
      "Epoch 393\n",
      "tau 0.400\n",
      "Learning rate 0.11419 * 10^-3\n",
      "Reward: 0.6749999999999996\n",
      "Entropy Reward: 0.4272729648975656\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6450000000000004\n",
      "\n",
      "Epoch 442\n",
      "tau 0.400\n",
      "Learning rate 0.08743 * 10^-3\n",
      "Reward: 0.6800000000000003\n",
      "Entropy Reward: 0.4305530716094655\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6925000000000009\n",
      "\n",
      "Epoch 491\n",
      "tau 0.400\n",
      "Learning rate 0.07371 * 10^-3\n",
      "Reward: 0.6700000000000002\n",
      "Entropy Reward: 0.4375190752866911\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.5850000000000006\n",
      "\n",
      "Epoch 540\n",
      "tau 0.400\n",
      "Learning rate 0.06249 * 10^-3\n",
      "Reward: 0.5750000000000002\n",
      "Entropy Reward: 0.4059700893936681\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6400000000000007\n",
      "\n",
      "Epoch 589\n",
      "tau 0.400\n",
      "Learning rate 0.04929 * 10^-3\n",
      "Reward: 0.6750000000000008\n",
      "Entropy Reward: 0.44284341789316384\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7000000000000004\n",
      "\n",
      "Epoch 638\n",
      "tau 0.400\n",
      "Learning rate 0.04235 * 10^-3\n",
      "Reward: 0.5625000000000003\n",
      "Entropy Reward: 0.41989219682291157\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7237500000000003\n",
      "\n",
      "Epoch 687\n",
      "tau 0.400\n",
      "Learning rate 0.03404 * 10^-3\n",
      "Reward: 0.6725000000000009\n",
      "Entropy Reward: 0.4684838401060551\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6187500000000006\n",
      "\n",
      "Epoch 736\n",
      "tau 0.400\n",
      "Learning rate 0.02961 * 10^-3\n",
      "Reward: 0.6875000000000003\n",
      "Entropy Reward: 0.42920941326301554\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6812500000000009\n",
      "\n",
      "Epoch 785\n",
      "tau 0.400\n",
      "Learning rate 0.02423 * 10^-3\n",
      "Reward: 0.7400000000000002\n",
      "Entropy Reward: 0.4201760408910924\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6575000000000003\n",
      "\n",
      "Epoch 834\n",
      "tau 0.400\n",
      "Learning rate 0.02131 * 10^-3\n",
      "Reward: 0.6575000000000004\n",
      "Entropy Reward: 0.471385738956742\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6575000000000004\n",
      "\n",
      "Epoch 883\n",
      "tau 0.400\n",
      "Learning rate 0.01772 * 10^-3\n",
      "Reward: 0.6725000000000004\n",
      "Entropy Reward: 0.42243780342629195\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7750000000000012\n",
      "\n",
      "Epoch 932\n",
      "tau 0.400\n",
      "Learning rate 0.01575 * 10^-3\n",
      "Reward: 0.7075000000000002\n",
      "Entropy Reward: 0.4437531038583257\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7300000000000002\n",
      "\n",
      "Epoch 981\n",
      "tau 0.400\n",
      "Learning rate 0.01329 * 10^-3\n",
      "Reward: 0.6950000000000005\n",
      "Entropy Reward: 0.43260053919395436\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6900000000000011\n",
      "\n",
      "Epoch 1030\n",
      "tau 0.400\n",
      "Learning rate 0.01193 * 10^-3\n",
      "Reward: 0.7300000000000005\n",
      "Entropy Reward: 0.4552978666096169\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7125000000000006\n",
      "\n",
      "Epoch 1079\n",
      "tau 0.400\n",
      "Learning rate 0.01074 * 10^-3\n",
      "Reward: 0.6249999999999999\n",
      "Entropy Reward: 0.4290623479202623\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6812500000000001\n",
      "\n",
      "Epoch 1128\n",
      "tau 0.400\n",
      "Learning rate 0.00923 * 10^-3\n",
      "Reward: 0.7050000000000004\n",
      "Entropy Reward: 0.4677432721055811\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.6574999999999995\n",
      "\n",
      "Epoch 1177\n",
      "tau 0.400\n",
      "Learning rate 0.00839 * 10^-3\n",
      "Reward: 0.6925000000000008\n",
      "Entropy Reward: 0.4606572464518467\n",
      "Calculated until horizon 7\n",
      "Optimal entropy solution: 0.47971497176851524\n",
      "CK full rank: True\n",
      "Test reward: 0.7300000000000009\n",
      "\n",
      "Calculated until horizon 7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MTRNet Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent4.tau = 0.4\n",
    "agent4.lightMTR = True\n",
    "agent4.lr = LEARNING_RATE\n",
    "agent4.policy.ntk_init()\n",
    "_,_,data4 = train_agent(agent4, env, num_epoches=NUM_EPOCHES*3, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent4.tau, lr_end=LR_END, patience=PATIENCE, clip_grad=True, testing=True)\n",
    "\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data4_ = test_agent(agent4, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES, tau=agent4.tau)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "df =push(df, data4, data4_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "ck = Ntk_analysis(env)\n",
    "ck.full_analysis(agent4, mode = \"ck\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1': 8, '2': 8, '3': 8, '4': 8}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck.rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "load(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
