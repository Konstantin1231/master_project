{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "        SETUP ENVIRONMENT\n",
    "\n",
    "Choose a game:\n",
    "- \"Cart\" # ok for 100 epoch\n",
    "- \"Car\" # not good choice\n",
    "- \"Pendulum\" # not good choice\n",
    "- \"Lake\" # demand more then 1500 epoch\n",
    "- \"Maze\" # 100 epoch is OK\n",
    "- \"Toy\" # coming ...\n",
    "\"\"\"\n",
    "GAME_NAME = \"Cart\"\n",
    "\"\"\"\n",
    "        SETUP AGENT and TRAINING (FEED FORWARD)\n",
    "\"\"\"\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "BETA_REIN_1 = 1 #Betas are scalar by which we multiply bias terms in NN at initialization.\n",
    "BETA_REIN_2 = 0.1 #They are used to control Chaos order.\n",
    "LEARNING_RATE_REIN = 0.0001\n",
    "HORIZON_REIN = 100\n",
    "HIDDEN_DIM_REIN = 32\n",
    "\"\"\"MATRYOSHKA\"\"\"\n",
    "BETA_MTR = 0.1\n",
    "LEARNING_RATE_MTR = 0.0001\n",
    "HORIZON_MTR= 100\n",
    "HIDDEN_DIM_MTR = 32\n",
    "TAU = 0.7\n",
    "\"\"\"Training\"\"\"\n",
    "NUM_EPOCHES = 600\n",
    "N_EPISODES = 10 # number of episodes per epoch. Used for both: Reinforce and Matryoshka"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )\n",
    "from mtrshka import ReinforceAgent\n",
    "agent1 = ReinforceAgent( obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name=GAME_NAME)\n",
    "agent1.policy.ntk_init(beta=BETA_REIN_1)\n",
    "agent2 = ReinforceAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name= GAME_NAME)\n",
    "agent2.policy.ntk_init(beta=BETA_REIN_2)\n",
    "from mtrshka import MTRAgent\n",
    "agent = MTRAgent(obs_dim, action_dim, hidden_dim= HIDDEN_DIM_MTR , horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_MTR, game_name= GAME_NAME, tau=TAU)\n",
    "agent.policy.ntk_init(beta=BETA_MTR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE)\n",
    "        Remark: we are training two reinforce agents\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import run_episodes\n",
    "import matplotlib.pyplot as plt\n",
    "env_reset(env)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "loss_list_1 = []\n",
    "loss_list_2 = []\n",
    "# Train for a number of epochs\n",
    "print(f\"beta1 = {BETA_REIN_1}    beta2 = {BETA_REIN_2}\")\n",
    "epsilon = np.linspace(0.3,0.3,NUM_EPOCHES)\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    episodes = run_episodes(agent1, env, n_episodes=N_EPISODES,  game= GAME_NAME, epsilon = 0)  # Collect episodes\n",
    "    episodes_ntk = run_episodes(agent2, env, n_episodes=N_EPISODES,  game= GAME_NAME, epsilon = 0)  # Collect episodes\n",
    "    loss_1 = agent1.train( episodes, clip_grad= True)  # Update the policy based on the episodes\n",
    "    loss_2 = agent2.train(episodes_ntk, clip_grad= True)  # Update the policy based on the episodes\n",
    "    loss_list_1.append(loss_1)\n",
    "    loss_list_2.append(loss_2)\n",
    "    if epoch%20 == 0:\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print(f'Reward: {loss_1}  and  {loss_2}')\n",
    "    env_reset(env)\n",
    "close_env(env)\n",
    "ax1.scatter(range(len(loss_list_1)),loss_list_1, label=f\"{BETA_REIN_1}\")\n",
    "ax1.scatter(range(len(loss_list_1)),loss_list_2, label=f\"{BETA_REIN_2}\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Matryoshka Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import run_episodes_mtr\n",
    "env_reset(env)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "loss_list_mtr = []\n",
    "# Train for a number of epochs\n",
    "print(f\"beta = {BETA_MTR} \")\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    #agent.tau = np.sin( 11 * epoch / NUM_EPOCHES  )/3  + 0.6\n",
    "    episodes = run_episodes_mtr(agent, env, n_episodes=N_EPISODES, game = GAME_NAME)  # Collect episodes\n",
    "    loss = agent.train(episodes)  # Update the policy based on the episodes\n",
    "    loss_list_mtr.append(loss)\n",
    "    if epoch%20 == 0:\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print(f'Reward: {loss} ')\n",
    "    env_reset(env)\n",
    "close_env(env)\n",
    "ax1.scatter(range(len(loss_list_mtr)),loss_list_mtr, label=f\"{BETA_MTR}\")\n",
    "ax1.grid()\n",
    "ax1.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        COMPARE REINFORCE VS MTR\n",
    "        Remark: Run if you have trained both: Reinforce and Matryoshka.\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.scatter(range(len(loss_list_mtr)),loss_list_mtr, label=f\"MTR: BETA = {BETA_MTR}, TAU = {agent.tau}\")\n",
    "ax1.scatter(range(len(loss_list_1)),loss_list_1, label=f\"REIN: BETA = {BETA_REIN_1}\")\n",
    "ax1.scatter(range(len(loss_list_1)),loss_list_2, label=f\"REIN: BETA = {BETA_REIN_2}\")\n",
    "ax1.set_title(f\"REINFORCE VS MATRYOSHKA \\n Horizon = {HORIZON_MTR}; Game = {GAME_NAME}; NN = Full Connected\")\n",
    "ax1.set_ylabel(\"Total Reward\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "plt.savefig(f'images/{GAME_NAME}.jpg', format='jpeg')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TEST AGENT REINFORCE\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import game_setup\n",
    "env, _, _  = game_setup(GAME_NAME, render = True)\n",
    "run_episodes(agent1, env, n_episodes=3)\n",
    "env.reset()\n",
    "run_episodes(agent2, env, n_episodes=3)\n",
    "close_env(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TEST AGENT MATRYOSHKA\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent.tau = 0.001\n",
    "from enviroment import game_setup\n",
    "env, _, _  = game_setup(GAME_NAME, render = True)\n",
    "for epoch in range(3):\n",
    "    episodes = run_episodes_mtr(agent, env, n_episodes=1, game= GAME_NAME)  # Collect episodes\n",
    "    env_reset(env)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE) PARALLEL\n",
    "        Remark: NOT WORKING, in process...\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 30\n",
    "N_EPISODES = 20 # ^number of episodes per epoch\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "import torch.optim as optim\n",
    "from try_grad_policy import train_policy\n",
    "import multiprocessing\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "optimizer_ntk = optim.Adam(ntk_policy.parameters(), lr=LEARNING_RATE)  # Define the optimizer\n",
    "\n",
    "\n",
    "initialize_env(env)\n",
    "\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "run_process1 = multiprocessing.Process(target=run_episodes, args=(policy, env,output_queue1, N_EPISODES))\n",
    "run_process2 = multiprocessing.Process(target=run_episodes, args=(ntk_policy, env,output_queue2, N_EPISODES))\n",
    "\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(NUM_EPOCHES ):\n",
    "    run_process1.start()  # Collect episodes\n",
    "    run_process2.start()\n",
    "    # Join the processes to wait for them to finish\n",
    "    run_process1.join()\n",
    "    run_process2.join()\n",
    "    # Get the outputs from both processes\n",
    "    episodes = output_queue1.get()\n",
    "    ntk_episodes = output_queue2.get()\n",
    "    loss = train_policy(policy, optimizer, episodes)  # Update the policy based on the episodes\n",
    "    ntk_loss = train_policy(ntk_policy, optimizer_ntk, ntk_episodes)  # Update the ntk_policy based on the episodes\n",
    "    print(f'Episode {epoch + 1}, Total Loss: {BETA1}={loss} and {BETA2}={ntk_loss} ')\n",
    "close_env(env)\n",
    "\"-------------------TEST-------------------\"\n",
    "env = gym.make('CartPole-v1', render_mode = \"human\" )\n",
    "run_episodes(policy, env, n_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
