{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_64788\\841550308.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \"\"\"\n\u001b[0;32m     19\u001b[0m \u001b[1;34m\"\"\"-------------------------------------------------------------------\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0menviroment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RL\\enviroment.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmaze_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCustomMazeEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtoy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mToy_env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "        SETUP ENVIRONMENT\n",
    "\n",
    "Choose a game:\n",
    "- \"Cart\" # ok for 100 epoch\n",
    "- \"Car\" # not good choice\n",
    "- \"Pendulum\" # not good choice\n",
    "- \"Lake\" # demand more then 1500 epoch\n",
    "- \"Maze\" # 100 epoch is OK\n",
    "- \"Toy\" # 100 epoch\n",
    "\"\"\"\n",
    "GAME_NAME = \"Lake\"\n",
    "HORIZON = 10 #fixed horizon\n",
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "from utils import train_agent\n",
    "import pandas as pd\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )\n",
    "env.rnd_init_state = True\n",
    "env.testing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                 SETUP AGENT and TRAINING\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "HIDDEN_DIM_REIN = 24\n",
    "EPSILON = 0.1 # for epsilon greedy policy\n",
    "\"\"\"MATRYOSHKA Hidden layer\"\"\"\n",
    "HIDDEN_DIM_MTR = HIDDEN_DIM_REIN\n",
    "\"\"\"ResNet\"\"\"\n",
    "HIDDEN_DIM_RESNET = 10\n",
    "#MTRNet with dynamically changing parameters number per layer\n",
    "INIT_HIDDEN_LAYER = 20\n",
    "\n",
    "\"\"\"Training\"\"\"\n",
    "TAU = 0.9\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHES = 600\n",
    "N_EPISODES = 25 # number of episodes per epoch.\n",
    "\n",
    "\"\"\"Testing\"\"\"\n",
    "NUM_TEST_EPISODES = 80\n",
    "\n",
    "\n",
    "\"\"\"DYNAMICAL TAU AND LR\"\"\"\n",
    "PATIENCE = 20 # for dynamical tau and lr\n",
    "TAU_END = TAU # for dynamical tau\n",
    "LR_END = LEARNING_RATE / 100 # for dynamical learning rate\n",
    "\"\"\"\n",
    "                    INITIALIZING AGENTS\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "#more settings---------------------\n",
    "HORIZON_REIN = HORIZON\n",
    "HORIZON_MTR = HORIZON\n",
    "LEARNING_RATE_REIN = LEARNING_RATE\n",
    "LEARNING_RATE_MTR  = LEARNING_RATE\n",
    "LEARNING_RATE_RESNET = LEARNING_RATE\n",
    "LEARNING_RATE_ORIGINAL = LEARNING_RATE\n",
    "#-------------------------------------\n",
    "HIDDEN_DIM_ORIGINAL = HIDDEN_DIM_RESNET\n",
    "from original import OriginalMtrAgent\n",
    "from NeuralNet import ReinforceAgent, MTRAgent\n",
    "from MtrNet import ReinforceMtrNetAgent, MtrNetAgent, ShortLongAgent\n",
    "\n",
    "#Reinforce Full Connected\n",
    "agent1 = ReinforceAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name=GAME_NAME)\n",
    "agent1.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#Reinforce MTRNet (Custom ResNet)\n",
    "agent2 = ReinforceMtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU)\n",
    "agent2.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#MTR extra-dimension\n",
    "agent3 = MTRAgent(obs_dim, action_dim, hidden_dim= HIDDEN_DIM_MTR , horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_MTR, game_name= GAME_NAME, tau=TAU)\n",
    "agent3.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#MTR with MTRNet\n",
    "agent4 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU)\n",
    "agent4.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#Original MTR\n",
    "agent5 = OriginalMtrAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_ORIGINAL,horizon= HORIZON, learning_rate= LEARNING_RATE_ORIGINAL , game_name= GAME_NAME, tau=TAU)\n",
    "agent5.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#MTR with MTRNet and dynamical number of parameter per layer\n",
    "agent6 = MtrNetAgent(obs_dim,action_dim, hidden_dim=INIT_HIDDEN_LAYER, horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, dynamical_layer_param=True)\n",
    "agent6.policy.ntk_init(sigma_w=0,sigma_b=0)\n",
    "\n",
    "#ShortLongNet\n",
    "agent7 = ShortLongAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU)\n",
    "agent7.policy.ntk_init(sigma_w=0,sigma_b=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DataFrame to store results\"\"\"\n",
    "FROM_PATH = \"results/Lake.csv\"\n",
    "TO_PATH = \"results/Lake.csv\"\n",
    "df = pd.read_csv(FROM_PATH)\n",
    "data1 = {\"init\": \"eoc\", 'description': \"Lake 8x8. One reward +30, each step reward -1, hole reward -5. Epoch: 600. Rein epsilon 0.15 \"}\n",
    "\n",
    "# function to push data\n",
    "def push(df,data2,data3,data1=data1):\n",
    "    data1.update(data2)\n",
    "    data1.update(data3)\n",
    "    df = df._append(data1, ignore_index=True)\n",
    "    return df\n",
    "#load to csv file path\n",
    "def load(df,path=TO_PATH):\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = {\\n    \\'index\\': 0,\\n    \\'game\\': \"Lol\",\\n    \\'agent\\': \"None\",\\n    \\'init\\': \"eoc\",\\n    \\'lr\\': 0.00,\\n    \\'tau\\': 0.00,\\n    \\'test\\': 0.00,\\n    \\'train\\': 0.00,\\n    \\'full_rank\\': 0,\\n    \\'total_param\\': 0,\\n    \\'description\\': None,\\n}\\ndf = df._append(data, ignore_index=True)\\ndf = pd.DataFrame(data, index=[0])\\ndf.to_csv(\"results.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = {\n",
    "    'index': 0,\n",
    "    'game': \"Lol\",\n",
    "    'agent': \"None\",\n",
    "    'init': \"eoc\",\n",
    "    'lr': 0.00,\n",
    "    'tau': 0.00,\n",
    "    'test': 0.00,\n",
    "    'train': 0.00,\n",
    "    'full_rank': 0,\n",
    "    'total_param': 0,\n",
    "    'description': None,\n",
    "}\n",
    "df = df._append(data, ignore_index=True)\n",
    "df = pd.DataFrame(data, index=[0])\n",
    "df.to_csv(\"results.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE CLASSIC)\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "\"\"\"Cart best for lr = 0.001\"\"\"\n",
    "\"\"\"For Maze Zero init work bad, but eoc good\"\"\"\n",
    "from utils import train_agent\n",
    "epsilon = EPSILON\n",
    "agent1.policy.ntk_init()\n",
    "loss_list, entropy_loss_list, data = train_agent(agent1, env, num_epoches=NUM_EPOCHES, game_name=GAME_NAME, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, epsilon=epsilon, clip_grad=True)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "score, data_ = test_agent(agent1,env,GAME_NAME, n_episodes=NUM_TEST_EPISODES)\n",
    "\"\"\"PUSH DATA\"\"\"\n",
    "df = push(df, data, data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 51\n",
      "tau 0.500\n",
      "Learning rate 0.81677 * 10^-3\n",
      "Reward: 0.28\n",
      "Entropy Reward: -3.490720672607422\n",
      "\n",
      "Epoch 101\n",
      "tau 0.500\n",
      "Learning rate 0.61297 * 10^-3\n",
      "Reward: -4.76\n",
      "Entropy Reward: -9.279319629669189\n",
      "\n",
      "Epoch 151\n",
      "tau 0.500\n",
      "Learning rate 0.51154 * 10^-3\n",
      "Reward: -0.84\n",
      "Entropy Reward: -4.777075996398926\n",
      "\n",
      "Epoch 201\n",
      "tau 0.500\n",
      "Learning rate 0.39579 * 10^-3\n",
      "Reward: -0.84\n",
      "Entropy Reward: -4.777075996398926\n",
      "\n",
      "Epoch 251\n",
      "tau 0.500\n",
      "Learning rate 0.33671 * 10^-3\n",
      "Reward: -0.84\n",
      "Entropy Reward: -4.777075996398926\n",
      "\n",
      "Epoch 301\n",
      "tau 0.500\n",
      "Learning rate 0.26772 * 10^-3\n",
      "Reward: -0.28\n",
      "Entropy Reward: -4.133898334503174\n",
      "\n",
      "Epoch 351\n",
      "tau 0.500\n",
      "Learning rate 0.23171 * 10^-3\n",
      "Reward: -3.08\n",
      "Entropy Reward: -7.3497866439819335\n",
      "\n",
      "Epoch 401\n",
      "tau 0.500\n",
      "Learning rate 0.18878 * 10^-3\n",
      "Reward: -4.2\n",
      "Entropy Reward: -8.636141967773437\n",
      "\n",
      "Epoch 451\n",
      "tau 0.500\n",
      "Learning rate 0.16591 * 10^-3\n",
      "Reward: -0.28\n",
      "Entropy Reward: -4.133898334503174\n",
      "\n",
      "Epoch 501\n",
      "tau 0.500\n",
      "Learning rate 0.13815 * 10^-3\n",
      "Reward: -2.52\n",
      "Entropy Reward: -6.706608982086181\n",
      "\n",
      "Epoch 551\n",
      "tau 0.500\n",
      "Learning rate 0.12309 * 10^-3\n",
      "Reward: -4.76\n",
      "Entropy Reward: -9.279319629669189\n",
      "\n",
      "Epoch 601\n",
      "tau 0.500\n",
      "Learning rate 0.10451 * 10^-3\n",
      "Reward: -0.84\n",
      "Entropy Reward: -4.777075996398926\n",
      "\n",
      "Epoch 651\n",
      "tau 0.500\n",
      "Learning rate 0.09427 * 10^-3\n",
      "Reward: -2.52\n",
      "Entropy Reward: -6.706608982086181\n",
      "\n",
      "Epoch 701\n",
      "tau 0.500\n",
      "Learning rate 0.08144 * 10^-3\n",
      "Reward: -3.64\n",
      "Entropy Reward: -7.992964305877686\n",
      "\n",
      "Epoch 751\n",
      "tau 0.500\n",
      "Learning rate 0.07427 * 10^-3\n",
      "Reward: -0.84\n",
      "Entropy Reward: -4.777075996398926\n",
      "\n",
      "Epoch 801\n",
      "tau 0.500\n",
      "Learning rate 0.06517 * 10^-3\n",
      "Reward: -3.64\n",
      "Entropy Reward: -7.992964305877686\n",
      "\n",
      "Epoch 851\n",
      "tau 0.500\n",
      "Learning rate 0.06001 * 10^-3\n",
      "Reward: -2.52\n",
      "Entropy Reward: -6.706608982086181\n",
      "\n",
      "Epoch 901\n",
      "tau 0.500\n",
      "Learning rate 0.05340 * 10^-3\n",
      "Reward: -4.2\n",
      "Entropy Reward: -8.636141967773437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koq1231\\AppData\\Local\\Temp\\ipykernel_17356\\79008672.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = df._append(data1, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Matryoshka EXTRA-DIMENSION Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent3.tau = 0.5\n",
    "agent3.lr = LEARNING_RATE\n",
    "agent3.policy.ntk_init()\n",
    "_,_,data3 = train_agent(agent3, env, num_epoches=3*NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent3.tau, lr_end=LR_END, patience=PATIENCE, clip_grad=False)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data3_ = test_agent(agent3,env,GAME_NAME, n_episodes=NUM_TEST_EPISODES)\n",
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data3, data3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 51\n",
      "tau 0.900\n",
      "Learning rate 0.73942 * 10^-3\n",
      "Reward: -2.96\n",
      "Entropy Reward: -4.147830951288342\n",
      "\n",
      "Epoch 101\n",
      "tau 0.900\n",
      "Learning rate 0.48771 * 10^-3\n",
      "Reward: 16.16\n",
      "Entropy Reward: 10.922254571914673\n",
      "\n",
      "Epoch 151\n",
      "tau 0.900\n",
      "Learning rate 0.37800 * 10^-3\n",
      "Reward: 17.44\n",
      "Entropy Reward: 11.841959607601165\n",
      "\n",
      "Epoch 201\n",
      "tau 0.900\n",
      "Learning rate 0.26604 * 10^-3\n",
      "Reward: 20.84\n",
      "Entropy Reward: 14.443393606543541\n",
      "\n",
      "Epoch 251\n",
      "tau 0.900\n",
      "Learning rate 0.21455 * 10^-3\n",
      "Reward: 23.76\n",
      "Entropy Reward: 16.678531579971313\n",
      "\n",
      "Epoch 301\n",
      "tau 0.900\n",
      "Learning rate 0.15950 * 10^-3\n",
      "Reward: 23.64\n",
      "Entropy Reward: 16.924810514450073\n",
      "\n",
      "Epoch 351\n",
      "tau 0.900\n",
      "Learning rate 0.13302 * 10^-3\n",
      "Reward: 23.76\n",
      "Entropy Reward: 16.661598176956176\n",
      "\n",
      "Epoch 401\n",
      "tau 0.900\n",
      "Learning rate 0.10357 * 10^-3\n",
      "Reward: 23.68\n",
      "Entropy Reward: 16.619761486053466\n",
      "\n",
      "Epoch 451\n",
      "tau 0.900\n",
      "Learning rate 0.08885 * 10^-3\n",
      "Reward: 23.76\n",
      "Entropy Reward: 16.658895907402037\n",
      "\n",
      "Epoch 501\n",
      "tau 0.900\n",
      "Learning rate 0.07193 * 10^-3\n",
      "Reward: 25.0\n",
      "Entropy Reward: 17.56300151348114\n",
      "\n",
      "Epoch 551\n",
      "tau 0.900\n",
      "Learning rate 0.06321 * 10^-3\n",
      "Reward: 22.28\n",
      "Entropy Reward: 15.807012195587157\n",
      "\n",
      "Epoch 601\n",
      "tau 0.900\n",
      "Learning rate 0.05288 * 10^-3\n",
      "Reward: 25.0\n",
      "Entropy Reward: 17.557499170303345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MTRNet Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent4.tau = 0.9\n",
    "agent4.policy.ntk_init()\n",
    "agent4.lightMTR = False\n",
    "_,_,data4 = train_agent(agent4, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent4.tau , lr_end=LR_END, patience=PATIENCE, clip_grad=True, testing=False)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data4_ = test_agent(agent4, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data4, data4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"agent\"]==\"MtrNet\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 51\n",
      "tau 0.700\n",
      "Learning rate 0.08333 * 10^-3\n",
      "Reward: 8.0\n",
      "Entropy Reward: 4.228945220510165\n",
      "\n",
      "Epoch 101\n",
      "tau 0.700\n",
      "Learning rate 0.06425 * 10^-3\n",
      "Reward: 7.333333333333333\n",
      "Entropy Reward: 3.758174334963163\n",
      "\n",
      "Epoch 151\n",
      "tau 0.700\n",
      "Learning rate 0.05448 * 10^-3\n",
      "Reward: 9.333333333333334\n",
      "Entropy Reward: 6.318525188912948\n",
      "\n",
      "Epoch 201\n",
      "tau 0.700\n",
      "Learning rate 0.04307 * 10^-3\n",
      "Reward: 9.666666666666666\n",
      "Entropy Reward: 5.900981002797683\n",
      "\n",
      "Epoch 251\n",
      "tau 0.700\n",
      "Learning rate 0.03711 * 10^-3\n",
      "Reward: 8.666666666666666\n",
      "Entropy Reward: 5.375679997106393\n",
      "\n",
      "Epoch 301\n",
      "tau 0.700\n",
      "Learning rate 0.03000 * 10^-3\n",
      "Reward: 9.0\n",
      "Entropy Reward: 5.89129956625402\n",
      "\n",
      "Epoch 351\n",
      "tau 0.700\n",
      "Learning rate 0.02622 * 10^-3\n",
      "Reward: 8.333333333333334\n",
      "Entropy Reward: 5.333344016224146\n",
      "\n",
      "Epoch 401\n",
      "tau 0.700\n",
      "Learning rate 0.02164 * 10^-3\n",
      "Reward: 8.666666666666666\n",
      "Entropy Reward: 4.82641786026458\n",
      "\n",
      "Epoch 451\n",
      "tau 0.700\n",
      "Learning rate 0.01916 * 10^-3\n",
      "Reward: 7.666666666666667\n",
      "Entropy Reward: 3.9249093358715377\n",
      "\n",
      "Epoch 501\n",
      "tau 0.700\n",
      "Learning rate 0.01610 * 10^-3\n",
      "Reward: 8.0\n",
      "Entropy Reward: 4.119535026450952\n",
      "\n",
      "Epoch 551\n",
      "tau 0.700\n",
      "Learning rate 0.01443 * 10^-3\n",
      "Reward: 8.666666666666666\n",
      "Entropy Reward: 4.865559371421114\n",
      "\n",
      "Epoch 601\n",
      "tau 0.700\n",
      "Learning rate 0.01233 * 10^-3\n",
      "Reward: 8.666666666666666\n",
      "Entropy Reward: 5.007806923240423\n",
      "\n",
      "Epoch 651\n",
      "tau 0.700\n",
      "Learning rate 0.01116 * 10^-3\n",
      "Reward: 8.333333333333334\n",
      "Entropy Reward: 4.340839256842931\n",
      "\n",
      "Epoch 701\n",
      "tau 0.700\n",
      "Learning rate 0.00969 * 10^-3\n",
      "Reward: 7.666666666666667\n",
      "Entropy Reward: 3.9662914412717023\n",
      "\n",
      "Epoch 751\n",
      "tau 0.700\n",
      "Learning rate 0.00885 * 10^-3\n",
      "Reward: 8.333333333333334\n",
      "Entropy Reward: 4.489999309182167\n",
      "\n",
      "Epoch 801\n",
      "tau 0.700\n",
      "Learning rate 0.00779 * 10^-3\n",
      "Reward: 9.666666666666666\n",
      "Entropy Reward: 5.9526608648399515\n",
      "\n",
      "Epoch 851\n",
      "tau 0.700\n",
      "Learning rate 0.00718 * 10^-3\n",
      "Reward: 8.333333333333334\n",
      "Entropy Reward: 4.556467993805806\n",
      "\n",
      "Epoch 901\n",
      "tau 0.700\n",
      "Learning rate 0.00639 * 10^-3\n",
      "Reward: 7.666666666666667\n",
      "Entropy Reward: 3.864720256999135\n",
      "\n",
      "Epoch 951\n",
      "tau 0.700\n",
      "Learning rate 0.00594 * 10^-3\n",
      "Reward: 9.0\n",
      "Entropy Reward: 5.417305768157045\n",
      "\n",
      "Epoch 1001\n",
      "tau 0.700\n",
      "Learning rate 0.00535 * 10^-3\n",
      "Reward: 7.666666666666667\n",
      "Entropy Reward: 3.9078673819700875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MTRNet with dynamical number of parameters Training Dy\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent6.policy.ntk_init()\n",
    "agent6.lr = LEARNING_RATE\n",
    "agent6.tau = 0.7\n",
    "_,_,data6 = train_agent(agent6, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent6.tau , lr_end=LR_END, patience=PATIENCE, clip_grad=True)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data6_ = test_agent(agent6, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koq1231\\AppData\\Local\\Temp\\ipykernel_17356\\3362490508.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = df._append(data1, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data6, data6_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sigma_w = 1.4142135623730951\n",
      " sigma_b = 0\n",
      "Epoch 51\n",
      "tau 0.700\n",
      "Learning rate 0.08333 * 10^-3\n",
      "Reward: 14.333333333333334\n",
      "Entropy Reward: 13.641520530470492\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 20.55\n",
      "\n",
      "Epoch 101\n",
      "tau 0.700\n",
      "Learning rate 0.06425 * 10^-3\n",
      "Reward: 22.666666666666668\n",
      "Entropy Reward: 20.327831843119686\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.775\n",
      "\n",
      "Epoch 151\n",
      "tau 0.700\n",
      "Learning rate 0.05448 * 10^-3\n",
      "Reward: 13.333333333333334\n",
      "Entropy Reward: 11.750056453359624\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.7625\n",
      "\n",
      "Epoch 201\n",
      "tau 0.700\n",
      "Learning rate 0.04307 * 10^-3\n",
      "Reward: 12.0\n",
      "Entropy Reward: 10.656896777373428\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 20.9625\n",
      "\n",
      "Epoch 251\n",
      "tau 0.700\n",
      "Learning rate 0.03711 * 10^-3\n",
      "Reward: 19.333333333333332\n",
      "Entropy Reward: 17.669533358343568\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.1625\n",
      "\n",
      "Epoch 301\n",
      "tau 0.700\n",
      "Learning rate 0.03000 * 10^-3\n",
      "Reward: 18.333333333333332\n",
      "Entropy Reward: 16.06251238542609\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.5875\n",
      "\n",
      "Epoch 351\n",
      "tau 0.700\n",
      "Learning rate 0.02622 * 10^-3\n",
      "Reward: 15.666666666666666\n",
      "Entropy Reward: 14.859800771965334\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 21.1625\n",
      "\n",
      "Epoch 401\n",
      "tau 0.700\n",
      "Learning rate 0.02164 * 10^-3\n",
      "Reward: 14.333333333333334\n",
      "Entropy Reward: 12.910500637798881\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.75\n",
      "\n",
      "Epoch 451\n",
      "tau 0.700\n",
      "Learning rate 0.01916 * 10^-3\n",
      "Reward: 17.666666666666668\n",
      "Entropy Reward: 17.626968389493413\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.9875\n",
      "\n",
      "Epoch 501\n",
      "tau 0.700\n",
      "Learning rate 0.01610 * 10^-3\n",
      "Reward: 16.666666666666668\n",
      "Entropy Reward: 14.438091504140175\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 20.925\n",
      "\n",
      "Epoch 551\n",
      "tau 0.700\n",
      "Learning rate 0.01443 * 10^-3\n",
      "Reward: 17.0\n",
      "Entropy Reward: 15.048917488418132\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.975\n",
      "\n",
      "Epoch 601\n",
      "tau 0.700\n",
      "Learning rate 0.01233 * 10^-3\n",
      "Reward: 16.333333333333332\n",
      "Entropy Reward: 15.831638677394949\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.6375\n",
      "\n",
      "Epoch 651\n",
      "tau 0.700\n",
      "Learning rate 0.01116 * 10^-3\n",
      "Reward: 20.333333333333332\n",
      "Entropy Reward: 18.843100941429537\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.55\n",
      "\n",
      "Epoch 701\n",
      "tau 0.700\n",
      "Learning rate 0.00969 * 10^-3\n",
      "Reward: 16.666666666666668\n",
      "Entropy Reward: 14.842625955934636\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.3375\n",
      "\n",
      "Epoch 751\n",
      "tau 0.700\n",
      "Learning rate 0.00885 * 10^-3\n",
      "Reward: 16.333333333333332\n",
      "Entropy Reward: 15.75195555439374\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.1125\n",
      "\n",
      "Epoch 801\n",
      "tau 0.700\n",
      "Learning rate 0.00779 * 10^-3\n",
      "Reward: 15.666666666666666\n",
      "Entropy Reward: 14.282635591274206\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 20.4\n",
      "\n",
      "Epoch 851\n",
      "tau 0.700\n",
      "Learning rate 0.00718 * 10^-3\n",
      "Reward: 13.666666666666666\n",
      "Entropy Reward: 12.46790605930922\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.8375\n",
      "\n",
      "Epoch 901\n",
      "tau 0.700\n",
      "Learning rate 0.00639 * 10^-3\n",
      "Reward: 21.666666666666668\n",
      "Entropy Reward: 20.511953421945993\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 20.1875\n",
      "\n",
      "Epoch 951\n",
      "tau 0.700\n",
      "Learning rate 0.00594 * 10^-3\n",
      "Reward: 15.666666666666666\n",
      "Entropy Reward: 13.76266075084762\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 18.9875\n",
      "\n",
      "Epoch 1001\n",
      "tau 0.700\n",
      "Learning rate 0.00535 * 10^-3\n",
      "Reward: 30.666666666666668\n",
      "Entropy Reward: 28.79282171453815\n",
      "Optimal entropy solution: None\n",
      "CK full rank: None\n",
      "Test reward: 19.425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Matryoshka \"ORIGINAL\" Training\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent5.policy.ntk_init()\n",
    "agent5.tau = TAU\n",
    "agent5.lr = LEARNING_RATE\n",
    "_,_,data5 = train_agent(agent5, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=agent5.tau, lr_end=LR_END, patience=PATIENCE, clip_grad=False, testing=True)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data5_ = test_agent(agent5, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Ntk_analysis\n",
    "ntk = Ntk_analysis(env)\n",
    "ntk.full_analysis(agent5, mode=\"ntk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk.eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koq1231\\AppData\\Local\\Temp\\ipykernel_17356\\3362490508.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = df._append(data1, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data5, data5_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    MTRNet Light Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent4.tau = TAU\n",
    "agent4.lightMTR = True\n",
    "agent4.lr = LEARNING_RATE\n",
    "agent4.policy.ntk_init()\n",
    "_,_,data9 = train_agent(agent4, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, clip_grad=False)\n",
    "\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data9_ = test_agent(agent4, env, GAME_NAME, n_episodes=NUM_TEST_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data9, data9_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ShortLongNet Training\n",
    "    Remark: Horizon should be at least 10!\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "_,_,data7 = train_agent(agent7, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES,game_name=GAME_NAME, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, clip_grad=False)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data7_ = test_agent(agent7, env, GAME_NAME)\n",
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data7, data7_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE WITH MTR NN)\n",
    "        Remark: we are training two reinforce agents\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent2.lr = agent1.lr\n",
    "from utils import train_agent\n",
    "agent2.policy.ntk_init()\n",
    "_, _, data2 = train_agent(agent2, env, num_epoches=NUM_EPOCHES, game_name=GAME_NAME, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END , patience=PATIENCE, clip_grad=False)\n",
    "\"\"\"Testing\"\"\"\n",
    "from utils import test_agent\n",
    "_, data2_ = test_agent(agent2,env,GAME_NAME, n_episodes=NUM_TEST_EPISODES)\n",
    "\"\"\"PUSH DATA\"\"\"\n",
    "df =push(df, data2, data2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check the optimal solution\"\"\"\n",
    "env.generate_all_q_stars(3, tau = 0.05)\n",
    "env.v_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TEST AGENT (In render mode)\n",
    "\"\"\"\n",
    "agent = agent1\n",
    "NUMBER_OF_EPISODES = 1\n",
    "agent.tau = 0.05\n",
    "#env.generate_all_q_stars(horizon = HORIZON, tau = agent.tau)\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import render\n",
    "render(agent,env, NUMBER_OF_EPISODES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write .csv file to save DataFrame\"\"\"\n",
    "load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
