{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        SETUP ENVIRONMENT\n",
    "\n",
    "Choose a game:\n",
    "- \"Cart\" # ok for 100 epoch\n",
    "- \"Car\" # not good choice\n",
    "- \"Pendulum\" # not good choice\n",
    "- \"Lake\" # demand more then 1500 epoch\n",
    "- \"Maze\" # 100 epoch is OK\n",
    "- \"Toy\" # 100 epoch\n",
    "\"\"\"\n",
    "GAME_NAME = \"Toy\"\n",
    "\"\"\"\n",
    "            INITIALIZING AGENTS and ENVIRONMENT\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import *\n",
    "env, obs_dim, action_dim = game_setup(GAME_NAME )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                 SETUP AGENT and TRAINING\n",
    "\"\"\"\n",
    "\"\"\"REINFORCE\"\"\"\n",
    "BETA_REIN_FULL = 0.4 #Betas are scalar by which we multiply bias terms in NN at initialization. They are used to control Chaos order.\n",
    "BETA_REIN_RESNET = BETA_REIN_FULL\n",
    "LEARNING_RATE_REIN = 0.0001\n",
    "HORIZON_REIN = 7\n",
    "HIDDEN_DIM_REIN = 32\n",
    "\"\"\"MATRYOSHKA\"\"\"\n",
    "BETA_MTR_FULL = 0.1\n",
    "LEARNING_RATE_MTR = 0.0001\n",
    "HORIZON_MTR= HORIZON_REIN\n",
    "HIDDEN_DIM_MTR = 32\n",
    "TAU = 0.4\n",
    "\"\"\"ResNet\"\"\"\n",
    "BETA_MTR_RESNET = BETA_MTR_FULL\n",
    "HIDDEN_DIM_RESNET = 16\n",
    "LEARNING_RATE_RESNET = LEARNING_RATE_MTR * 10\n",
    "#MTRNet with dynamically changing parameters number per layer\n",
    "INIT_HIDDEN_LAYER = 10\n",
    "\"\"\"Training\"\"\"\n",
    "NUM_EPOCHES = 500\n",
    "N_EPISODES = 10 # number of episodes per epoch. Used for both: Reinforce\n",
    "\"\"\"DYNAMICAL TAU AND LR\"\"\"\n",
    "PATIENCE = 20 # for dynamical tau and lr\n",
    "TAU_END = 0.3 # for dynamical tau\n",
    "LR_END = LEARNING_RATE_MTR / 1000 # for dynamical learning rate\n",
    "\"\"\"\n",
    "                    INITIALIZING AGENTS\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from NeuralNet import ReinforceAgent, MTRAgent\n",
    "from MtrNet import ReinforceMtrNetAgent, MtrNetAgent, ShortLongAgent\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "print(device)\n",
    "#Reinforce Full Connected\n",
    "agent1 = ReinforceAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_REIN,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_REIN, game_name=GAME_NAME, beta = BETA_MTR_FULL)\n",
    "agent1.policy.ntk_init(beta=agent1.beta)\n",
    "\n",
    "#Reinforce MTRNet (Custom ResNet)\n",
    "agent2 = ReinforceMtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_REIN, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_REIN_RESNET)\n",
    "agent2.policy.ntk_init(beta=agent2.beta)\n",
    "\n",
    "#MTR extra-dimension\n",
    "agent3 = MTRAgent(obs_dim, action_dim, hidden_dim= HIDDEN_DIM_MTR , horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_MTR, game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_FULL)\n",
    "agent3.policy.ntk_init(beta=agent3.beta)\n",
    "\n",
    "#MTR with MTRNet\n",
    "agent4 = MtrNetAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET)\n",
    "agent4.policy.ntk_init(beta=agent4.beta)\n",
    "\n",
    "#MTR with MTRNet and dynamical number of parameter per layer\n",
    "agent6 = MtrNetAgent(obs_dim,action_dim, hidden_dim=INIT_HIDDEN_LAYER, horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET, dynamical_layer_param=True)\n",
    "agent6.policy.ntk_init(beta=agent6.beta)\n",
    "\n",
    "#ShortLongNet\n",
    "agent7 = ShortLongAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_RESNET,horizon= HORIZON_MTR, learning_rate= LEARNING_RATE_RESNET , game_name= GAME_NAME, tau=TAU, beta=BETA_MTR_RESNET)\n",
    "agent7.policy.ntk_init(beta=agent7.beta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE CLASSIC)\n",
    "        Remark: we are training two reinforce agents\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "loss_list_1 = train_agent(agent1, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TRAINING (REINFORCE WITH MTR NN)\n",
    "        Remark: we are training two reinforce agents\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "loss_list_2 = train_agent(agent2, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END * 10, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Matryoshka EXTRA-DIMENSION Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "loss_list_3 = train_agent(agent3, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MTRNet Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "loss_list_4 = train_agent(agent4, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END * 10, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MTRNet with dynamical number of parameters Training\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "loss_list_6 = train_agent(agent6, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END * 10, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ShortLongNet Training\n",
    "    Remark: Horizon should be at least 10!\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from utils import train_agent\n",
    "agent7.beta = 0.5\n",
    "agent7.lr = 0.1\n",
    "agent7.tau = 0.7\n",
    "agent7.set_optimazer()\n",
    "agent7.policy.ntk_init(agent7.beta)\n",
    "loss_list_7 = train_agent(agent7, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END*100 , patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Matryoshka \"ORIGINAL\" Training\n",
    "\"\"\"\n",
    "from original import OriginalMtrAgent\n",
    "# Set-up agent\n",
    "HIDDEN_DIM_ORIGINAL = 16\n",
    "HORIZON_ORIGINAL = 7\n",
    "BETA_ORIGINAL = 0.1\n",
    "TAU_ORIGINAL = 0.7\n",
    "LEARNING_RATE_ORIGINAL = LEARNING_RATE_RESNET\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "agent5 = OriginalMtrAgent(obs_dim,action_dim, hidden_dim=HIDDEN_DIM_ORIGINAL,horizon= HORIZON_ORIGINAL, learning_rate= LEARNING_RATE_ORIGINAL , game_name= GAME_NAME, tau=TAU_ORIGINAL, beta=BETA_ORIGINAL)\n",
    "agent5.policy.ntk_init(beta=agent5.beta)\n",
    "#training\n",
    "from utils import train_agent\n",
    "loss_list_5 = train_agent(agent5, env, num_epoches=NUM_EPOCHES, n_episodes=N_EPISODES, tau_end=TAU_END, lr_end=LR_END, patience=PATIENCE, clip_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        COMPARE REINFORCE VS MTR\n",
    "        Remark: Run if you have trained both: Reinforce and Matryoshka.\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.scatter(range(len(loss_list_1)),loss_list_1, label=f\"{agent1.name}: BETA = {agent1.beta}\")\n",
    "ax1.scatter(range(len(loss_list_2)),loss_list_2, label=f\"{agent2.name}: BETA = {agent2.beta}\")\n",
    "ax1.scatter(range(len(loss_list_3)),loss_list_3, label=f\"{agent3.name}: BETA = {agent3.beta}, TAU = {agent3.tau}\")\n",
    "ax1.scatter(range(len(loss_list_4)),loss_list_4, label=f\"{agent4.name}: BETA = {agent4.beta}, TAU = {agent4.tau}\")\n",
    "ax1.set_title(f\"Horizon = {HORIZON_MTR}; Game = {GAME_NAME}\")\n",
    "ax1.set_ylabel(\"Total Reward\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "plt.savefig(f'images/{GAME_NAME}_3.jpg', format='jpeg')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        COMPARE ORIGINAL VS MTRNet VS MTR extra-dim.\n",
    "        Remark: Run if you have trained both: Original and Matryoshka.\n",
    "\"\"\"\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.scatter(range(len(loss_list_5)),loss_list_5, label=f\"{agent5.name}: BETA = {agent5.beta}, TAU = {agent5.tau}\")\n",
    "ax1.scatter(range(len(loss_list_3)),loss_list_3, label=f\"{agent3.name}: BETA = {agent3.beta}, TAU = {agent3.tau}\")\n",
    "ax1.scatter(range(len(loss_list_4)),loss_list_4, label=f\"{agent4.name}: BETA = {agent4.beta}, TAU = {agent4.tau}\")\n",
    "ax1.set_title(f\"Horizon = {HORIZON_MTR}; Game = {GAME_NAME}\")\n",
    "ax1.set_ylabel(\"Total Reward\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "plt.savefig(f'images/{GAME_NAME}_6.jpg', format='jpeg')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        TEST AGENT\n",
    "\"\"\"\n",
    "NUMBER_OF_EPISODES = 5\n",
    "\"\"\"-------------------------------------------------------------------\"\"\"\n",
    "from enviroment import render\n",
    "render(agent1,env, NUMBER_OF_EPISODES)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
